{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712e94b1-fb68-4db4-a518-bc3baa6f7e21",
   "metadata": {},
   "source": [
    "El presente notebook se ha diseñado para la carga y uso del modelo ya entrenado, de manera que pueda comprobarse su funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9ecce-a7d0-4e98-8047-cac27b603121",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install protobuf\n",
    "!pip3 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68508014-6efc-48dc-9b16-81e05747618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmarrieta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Se realizan aquí las importaciones\n",
    "import import_ipynb\n",
    "import selenium_tests\n",
    "import yaml\n",
    "from screeninfo import get_monitors\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d65e6ad-e308-469f-8468-311b90182acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que el Software implementado está diseñado para pantallas de 4K, se obtienen con ayuda de esta función las pantallas disponibles y su resolución\n",
    "def obtener_info_pantallas():\n",
    "    monitores = get_monitors()\n",
    "\n",
    "    propiedades_monitor = None\n",
    "    ancho_monitor = 0\n",
    "    alto_monitor = 0\n",
    "    i = 0\n",
    "    while i < len(monitores):\n",
    "        if monitores[i].width > ancho_monitor and monitores[i].height > alto_monitor:\n",
    "            propiedades_monitor = monitores[i]\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    return propiedades_monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81587b75-ecc9-4c42-98c8-bf645c34316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Se obtienen las propiedades del monitor y Se crea un objeto de Tests, a cuyo constructor pasamos como parámetro las propiedades del monitor con mayor resolución\n",
    "#propiedades_monitor = obtener_info_pantallas()\n",
    "#tests = selenium_tests.Tests(propiedades_monitor)\n",
    "\n",
    "# Se configura el driver\n",
    "#tests.configurar_driver()\n",
    "\n",
    "# Se inicia la sesión\n",
    "#tests.iniciar_sesion(params={})\n",
    "\n",
    "#nombre_modelo = '.\\\\PLANTL-GOB-ES\\\\roberta-large-bne-massive-custom-lora'\n",
    "nombre_modelo = 'roberta-large-bne-massive_peft_fusionado'\n",
    "#nombre_tokenizador = 'roberta-large-bne-massive_peft_fusionado'\n",
    "nombre_tokenizador = './PLANTL-GOB-ES/roberta-large-bne-massive-custom-lora'\n",
    "\n",
    "#nombre_modelo = 'albert-base-10-spanish-finetuned-mldoc_peft_fusionado'\n",
    "#nombre_tokenizador = 'albert-base-10-spanish-finetuned-mldoc_peft_fusionado'\n",
    "#evaluate.load(\"accuracy\")\n",
    "modelo_roberta = AutoModelForSequenceClassification.from_pretrained(nombre_modelo, local_files_only=True)\n",
    "tokenizador_roberta = AutoTokenizer.from_pretrained(nombre_tokenizador, cache_dir='./huggingface_mirror', local_files_only=True, from_pt=True)\n",
    "pipe = pipeline('text-classification', modelo_roberta, tokenizer=tokenizador_roberta)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        prueba_a_realizar = input('\\nIndique qué desea hacer: ')\n",
    "        resultado = pipe(prueba_a_realizar)\n",
    "        puntuacion = resultado[0]['score']\n",
    "        print('Puntuación:', puntuacion)\n",
    "        prueba = resultado[0]['label']\n",
    "        \n",
    "        # Se comprueba si se trata de una o de varias pruebas\n",
    "        listado_pruebas = []\n",
    "        if prueba.startswith('[') is False:\n",
    "            print('ENTRA EN 1')\n",
    "            prueba_yaml = yaml.safe_load(prueba)\n",
    "            print(prueba_yaml)\n",
    "            listado_pruebas.append(prueba_yaml)\n",
    "            \n",
    "        else:\n",
    "            prueba = prueba[1:-1]\n",
    "            listado_pruebas = prueba.split('\\nURL:')\n",
    "\n",
    "            # Se eliminan posibles entradas sin información de la lista debido al split\n",
    "            while '' in listado_pruebas:\n",
    "                listado_pruebas.remove('')\n",
    "\n",
    "            # Se genera la lista con los YAML y crea el diccionario correspondiente para cada una de las pruebas a realizar\n",
    "            listado_pruebas = ['\\nURL:' + prueba for prueba in listado_pruebas]\n",
    "            listado_pruebas = [yaml.safe_load(prueba) for prueba in listado_pruebas]\n",
    "            \n",
    "        for prueba_a_realizar in listado_pruebas:\n",
    "            # Se va en primer lugar a la URL correspondiente. De ser la misma en que se encontraba no se ejecutarán acciones adicionales\n",
    "            #tests.ir_a_url(prueba_a_realizar['URL'])\n",
    "\n",
    "            # Se ejecutan a continuación las acciones requeridas\n",
    "            for accion in prueba_a_realizar['acciones']:\n",
    "                print('ACCION:', accion)\n",
    "                #funcion = getattr(tests, accion['funcion'])\n",
    "                #funcion(accion['params'])\n",
    "            \n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "    #tests.cerrar_sesion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a4b57e-4e17-47c1-82a4-014bc1225631",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m nombre_modelo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-large-bne-massive_peft_fusionado_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m nombre_tokenizador \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-large-bne-massive_peft_fusionado_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 14\u001b[0m modelo_roberta \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnombre_modelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m tokenizador_roberta \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(nombre_tokenizador, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./huggingface_mirror\u001b[39m\u001b[38;5;124m'\u001b[39m, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, from_pt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m'\u001b[39m, modelo_roberta, tokenizer\u001b[38;5;241m=\u001b[39mtokenizador_roberta)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:317\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:4946\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4939\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4940\u001b[0m is_from_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4943\u001b[0m     is_safetensors_available()\n\u001b[0;32m   4944\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_from_file\n\u001b[0;32m   4945\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded\n\u001b[1;32m-> 4946\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4947\u001b[0m ):\n\u001b[0;32m   4948\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(checkpoint_files[\u001b[38;5;241m0\u001b[39m], framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   4949\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mmetadata()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "# Se obtienen las propiedades del monitor y Se crea un objeto de Tests, a cuyo constructor pasamos como parámetro las propiedades del monitor con mayor resolución\n",
    "#propiedades_monitor = obtener_info_pantallas()\n",
    "#tests = selenium_tests.Tests(propiedades_monitor)\n",
    "\n",
    "# Se configura el driver\n",
    "#tests.configurar_driver()\n",
    "\n",
    "# Se inicia la sesión\n",
    "#tests.iniciar_sesion(params={})\n",
    "\n",
    "nombre_modelo = 'roberta-large-bne-massive_peft_fusionado_2'\n",
    "nombre_tokenizador = 'roberta-large-bne-massive_peft_fusionado_2'\n",
    "\n",
    "modelo_roberta = AutoModelForSequenceClassification.from_pretrained(nombre_modelo, local_files_only=True)\n",
    "tokenizador_roberta = AutoTokenizer.from_pretrained(nombre_tokenizador, cache_dir='./huggingface_mirror', local_files_only=True, from_pt=True)\n",
    "pipe = pipeline('text-classification', modelo_roberta, tokenizer=tokenizador_roberta)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        prueba_a_realizar = input('\\nIndique qué desea hacer: ')\n",
    "        resultado = pipe(prueba_a_realizar)\n",
    "        puntuacion = resultado[0]['score']\n",
    "        print('Puntuación:', puntuacion)\n",
    "        prueba = resultado[0]['label']\n",
    "        \n",
    "        # Se comprueba si se trata de una o de varias pruebas\n",
    "        listado_pruebas = []\n",
    "        if prueba.startswith('[') is False:\n",
    "            print('ENTRA EN 1')\n",
    "            prueba_yaml = yaml.safe_load(prueba)\n",
    "            print(prueba_yaml)\n",
    "            listado_pruebas.append(prueba_yaml)\n",
    "            \n",
    "        else:\n",
    "            prueba = prueba[1:-1]\n",
    "            listado_pruebas = prueba.split('\\nURL:')\n",
    "\n",
    "            # Se eliminan posibles entradas sin información de la lista debido al split\n",
    "            while '' in listado_pruebas:\n",
    "                listado_pruebas.remove('')\n",
    "\n",
    "            # Se genera la lista con los YAML y crea el diccionario correspondiente para cada una de las pruebas a realizar\n",
    "            listado_pruebas = ['\\nURL:' + prueba for prueba in listado_pruebas]\n",
    "            listado_pruebas = [yaml.safe_load(prueba) for prueba in listado_pruebas]\n",
    "            \n",
    "        for prueba_a_realizar in listado_pruebas:\n",
    "            # Se va en primer lugar a la URL correspondiente. De ser la misma en que se encontraba no se ejecutarán acciones adicionales\n",
    "            #tests.ir_a_url(prueba_a_realizar['URL'])\n",
    "\n",
    "            # Se ejecutan a continuación las acciones requeridas\n",
    "            for accion in prueba_a_realizar['acciones']:\n",
    "                print('ACCION:', accion)\n",
    "                #funcion = getattr(tests, accion['funcion'])\n",
    "                #funcion(accion['params'])\n",
    "            \n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "    #tests.cerrar_sesion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac246b-b626-4ac8-8bf1-d492a41379b6",
   "metadata": {},
   "source": [
    "Como se puede ver en la celda anterior, el intento de recuperación del modelo con mejores resultado para su uso en pruebas no se cargó correctamente, por lo que no fue posible comprobar su ejecución o posible valía para la ejecución de determinadas pruebas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
