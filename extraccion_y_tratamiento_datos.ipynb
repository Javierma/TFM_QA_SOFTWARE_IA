{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9849bad-ae46-4083-a43e-b100b0649a91",
   "metadata": {},
   "source": [
    "<h1><strong>Extracción y tratamiento de datos</strong></h1>\n",
    "\n",
    "El presente notebook busca tanto la extracción como el tratamiento de los datos a partir de los cuales construiremos el modelo. En primer lugar, se hará una extracción de los datos a partir de su ubicación en la herramienta IBM Doors siguiento el formato DXL (DOORS eXtension Language), un lenguaje de scripting propio del fabricante para la escritura, lectura y tratamiendo de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ed4a6-3e3c-4490-9209-a5ad940ed8be",
   "metadata": {},
   "source": [
    "En primer lugar realizaremos todas las importaciones de librerías necesarias para la ejecución del código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b66ae88-bff0-443c-b465-b2fe13425c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: IPython in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from import_ipynb) (8.15.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from import_ipynb) (5.10.4)\n",
      "Requirement already satisfied: backcall in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->import_ipynb) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->import_ipynb) (4.25.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->import_ipynb) (5.8.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.27.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (4.3.8)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (310)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->import_ipynb) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->IPython->import_ipynb) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->IPython->import_ipynb) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->IPython->import_ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from asttokens>=2.1.0->stack-data->IPython->import_ipynb) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->import_ipynb) (4.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.11)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (1.25.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (2.32.4)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.19.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmarrieta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip3 install import_ipynb\n",
    "!pip3 install nlpaug\n",
    "!pip3 install nltk\n",
    "!pip3 install sentencepiece\n",
    "\n",
    "import import_ipynb\n",
    "import selenium_tests\n",
    "import nlpaug.augmenter.word as aumentador_palabras\n",
    "import nlpaug.augmenter.sentence as aumentador_frases\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from screeninfo import get_monitors\n",
    "import time\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import subprocess\n",
    "\n",
    "import selenium.common.exceptions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.common.alert import Alert\n",
    "from selenium.webdriver.support.relative_locator import RelativeBy\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import nltk\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2846ff-2d5e-4a43-b289-ac2f87710d42",
   "metadata": {},
   "source": [
    "Para la extracción de los datos y guardado en fichero CSV puede consultarse el código escrito en el fichero <strong>exportacion_TP_CSV.dxl</strong>. Tras una serie de preguntas al usuario para la obtención de los datos requeridos, se hará una llamada al script dxl mencionado para la exportación de datos a CSV. Todo ello puede verse en la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33e4c768-8331-4984-8ea5-9dce80f5ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduzca la base de datos a la que se ha de conectar:  36677@NAVW1179.nav.es\n",
      "Introduzca su nombre de usuario:  jmarrieta\n",
      "Introduzca el nombre del software cuyos casos de prueba desea extraer:  IMPACT\n",
      "Introduzca el nombre del proyecto/versión:  IMPACT V2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/08/2025 18:51:25\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_01__TP_Generales_y_Arquitectura.csv...\n",
      "16/08/2025 18:51:25\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_02__TP_Base_de_datos_AIXM.csv...\n",
      "16/08/2025 18:51:25\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_03__TP_Acceso_a_IMPACT.csv...\n",
      "16/08/2025 18:51:26\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_04__TP_Escritorio.csv...\n",
      "16/08/2025 18:51:26\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_05__TP_General_componentes.csv...\n",
      "16/08/2025 18:51:26\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_06__TP_Histograma_Maestro.csv...\n",
      "16/08/2025 18:51:27\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_07__TP_Módulo_de_Histogramas_Auxiliares.csv...\n",
      "16/08/2025 18:51:29\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_08__TP_Gestor_de_medidas.csv...\n",
      "16/08/2025 18:51:31\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_09__TP_Listado_de_vuelos.csv...\n",
      "16/08/2025 18:51:31\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_10__TP_Módulo_multifuncional_general.csv...\n",
      "16/08/2025 18:51:31\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_11__TP_Explorador_de_medidas.csv...\n",
      "16/08/2025 18:51:31\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_12__TP_Configuración_operativa_de_sectores.csv...\n",
      "16/08/2025 18:51:32\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_13__TP_Configuración_operativa_de_pistas_de_aeródromo.csv...\n",
      "16/08/2025 18:51:32\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_14__TP_Módulo_de_gestión_de_complejidad.csv...\n",
      "16/08/2025 18:51:32\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_15__TP_Simulaciones.csv...\n",
      "16/08/2025 18:51:33\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_16__TP_FMP_Monitor.csv...\n",
      "16/08/2025 18:51:33\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_17__TP_Subcomponentes_generales_de_módulo.csv...\n",
      "16/08/2025 18:51:33\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_18__TP_Histograma_Auxiliar_sobre_Escritorio_Auxiliar.csv...\n",
      "16/08/2025 18:51:33\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_19__TP_Listado_de_vuelos_sobre_Escritorio_Auxiliar.csv...\n",
      "16/08/2025 18:51:34\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_20__TP_Configuración_operativa_planes_OTMV.csv...\n",
      "16/08/2025 18:51:34\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_21__TP_Configuración_operativa_plan_capacidad.csv...\n",
      "16/08/2025 18:51:34\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_22__TP_Configuración_operativa_planes_TV_Activation.csv...\n",
      "16/08/2025 18:51:34\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_23__TP_eHelpDesk_Tickets_y_REA.csv...\n",
      "16/08/2025 18:51:34\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_24__TP_Componente_multifuncional_en_escritorio_auxiliar.csv...\n",
      "16/08/2025 18:51:34\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\01_25__TP_Measure_Manager_en_escritorio_auxiliar.csv...\n",
      "16/08/2025 18:51:34\tProcesando C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\02_01__Pruebas_Regresión_Final_IMPACT_V2.csv...\n",
      "16/08/2025 18:51:47\tGuardando archivo C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\TPs\\..\\MODELO.csv\n"
     ]
    }
   ],
   "source": [
    "def obtener_cabecera_test(test):\n",
    "    return test.split('\\r\\n')[0]\n",
    "\n",
    "def generar_yaml(valor):\n",
    "    if valor == 'Case':\n",
    "        return 'URL: \\r\\n acciones: \\r\\n  - funcion:\\r\\n    params: '\n",
    "\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def obtener_titulos(tipos, tests):\n",
    "    titulos = []\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(tipos):\n",
    "        if tipos[i] == 'Test' or tipos[i] == '*':\n",
    "            titulos.append(tests[i])\n",
    "            j = i\n",
    "\n",
    "        elif tipos[i] == 'Case':\n",
    "            titulos.append(tests[j])\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    return titulos\n",
    "\n",
    "def obtener_dxl():\n",
    "    directorio_onedrive = os.getenv('OneDrive')\n",
    "    \n",
    "    # Creamos el directorio que contendrá los CSV exportados\n",
    "    directorio_exportacion = os.getcwd() + '\\\\ficheros_excel\\\\TPs\\\\'\n",
    "    os.makedirs(directorio_exportacion, exist_ok=True)\n",
    "    \n",
    "    # Se pregunta al usuario los datos para poder acceder a los módulos correspondientes dentro de IBM Doors\n",
    "    base_de_datos = input('Introduzca la base de datos a la que se ha de conectar: ')\n",
    "    usuario = input('Introduzca su nombre de usuario: ')\n",
    "    software = input('Introduzca el nombre del software cuyos casos de prueba desea extraer: ')\n",
    "    nombre_proyecto = input('Introduzca el nombre del proyecto/versión: ')\n",
    "    \n",
    "    # Dado que no es posible pasar parámetros al script DXL o no se ha sabido cómo, se guardarán los datos introducidos por el usuario\n",
    "    with open(directorio_onedrive + '\\Documents\\DOORS_y_CHANGE\\SCRIPTS DXL\\config_proyecto_sw.txt', mode='w') as f:\n",
    "        f.write(software + '\\n' + nombre_proyecto)\n",
    "    \n",
    "    # Hacemos la llamada al script exportacion_TP_CSV.dxl para la obtención de los ficheros que contienen las pruebas\n",
    "    comando_a_ejecutar = r'\"C:\\Program Files\\IBM\\Rational\\DOORS\\9.7\\bin\\doors.exe\" -d ' + base_de_datos + ' -user ' + usuario + ' -batch \"' + directorio_onedrive + '\\Documents\\DOORS_y_CHANGE\\SCRIPTS DXL\\TP_CSV_EXPORT.dxl\" -a \"\\\\' + base_de_datos.split('@')[-1] + '\\DXL\\addins;\\\\' + base_de_datos.split('@')[-1] + '\\DXL\" -J \\\\' + base_de_datos.split('@')[-1] + '\\DXL\\project\"'\n",
    "    resultado = subprocess.call(comando_a_ejecutar)\n",
    "\n",
    "    # Creamos el dataframe que contendrá todos los casos de prueba tras su lectura y procesamiento, así como las listas que contendrán las pruebas y los títulos\n",
    "    df_resultante = pd.DataFrame(columns=['Tipo', 'Test', 'Titulo'])\n",
    "    all_tests = []\n",
    "    all_titles = []\n",
    "\n",
    "    nombres_ficheros_csv = os.listdir(directorio_exportacion)\n",
    "\n",
    "    # Eliminamos de la lista todo aquel fichero que pueda existir en el directorio que no sea CSV\n",
    "    nombres_ficheros_csv = [nombre_fichero_csv for nombre_fichero_csv in nombres_ficheros_csv if nombre_fichero_csv.endswith('.csv')]\n",
    "\n",
    "    for nombre_ficheros_csv in nombres_ficheros_csv:\n",
    "        # Detectamos la codificación con que se guardaron, ya que los CSV contienen acentos y otros caracteres especiales\n",
    "        with open(directorio_exportacion + nombre_ficheros_csv, mode='rb') as f:\n",
    "            codificacion = chardet.detect(f.read())['encoding']\n",
    "\n",
    "        nombre_fichero = directorio_exportacion + nombre_ficheros_csv\n",
    "        with open(nombre_fichero, mode='r', encoding=codificacion) as f:\n",
    "            print(time.strftime('%d/%m/%Y %H:%M:%S') + '\\tProcesando ' + nombre_fichero + '...')\n",
    "            contenido_fichero = f.read()\n",
    "            contenido_fichero = contenido_fichero.split('\\n')\n",
    "\n",
    "            # Comprobamos si el caso de prueba aplica a la versión V2.01.13 o inferior. De no ser así, no exportará al CSV final\n",
    "            i = 0\n",
    "            while i < len(contenido_fichero):\n",
    "                if 'Case' in contenido_fichero[i]:\n",
    "                    j = i + 1\n",
    "                    tested_versions = []\n",
    "                    while j < len(contenido_fichero) and 'Case' not in contenido_fichero[j]:\n",
    "                        if 'Execution' in contenido_fichero[j]:\n",
    "                            tested_versions.append(contenido_fichero[j].split(';-;')[0])\n",
    "\n",
    "                        j = j + 1\n",
    "\n",
    "                    if '02.01.' not in ' '.join(tested_versions) and '02.00.' not in ' '.join(tested_versions):\n",
    "                        contenido_fichero = contenido_fichero[0:i] + contenido_fichero[j:]\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            # Eliminamos líneas que indican si la ejecución tuvo éxito o no\n",
    "            contenido_fichero = [content for content in contenido_fichero if ';-;Execution;-;' not in content]\n",
    "\n",
    "            # Eliminamos aquellas lineas referentes a tests sin casos de prueba\n",
    "            i = 0\n",
    "            while i < len(contenido_fichero):\n",
    "                if i < len(contenido_fichero) - 1 and ';Test;' in contenido_fichero[i] and ';Test;' in contenido_fichero[i + 1]:\n",
    "                    contenido_fichero.remove(contenido_fichero[i])\n",
    "\n",
    "                else:\n",
    "                    i = i + 1\n",
    "\n",
    "            # Check if the next line is part of a case, in which occurrence we add a \\n and write the value between double commas\n",
    "            # Comprobamos si la siguiente línea es parte de un caso de prueba. De ser así, añadimos una nueva línea y escribimos el texto entre comillas dobles para que en Excel sea una única celda\n",
    "            i = 2\n",
    "            while i < len(contenido_fichero):\n",
    "                if i < (len(contenido_fichero) - 1) and 'Case' not in contenido_fichero[i + 1] and 'Test' not in contenido_fichero[i + 1] and '*;' not in contenido_fichero[i + 1]:\n",
    "                    contenido_fichero[i] = contenido_fichero[i] + '\\r\\n' + contenido_fichero[i + 1]\n",
    "                    contenido_fichero.remove(contenido_fichero[i + 1])\n",
    "\n",
    "                else:\n",
    "                    i = i + 1\n",
    "\n",
    "            # Separamos las columnas. La separación original era ;-; para que las , o ; que pudiesen haber no se tomasen como separación de columnas\n",
    "            i = 0\n",
    "            while i < len(contenido_fichero):\n",
    "                contenido_fichero[i] = contenido_fichero[i].split(';-;')\n",
    "                if len(contenido_fichero[i]) > 1:\n",
    "                    contenido_fichero[i] = contenido_fichero[i][2:]\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        # Introducimos el contenido en un dataframe\n",
    "        df = pd.DataFrame(columns=contenido_fichero[0], data=contenido_fichero[1:])\n",
    "\n",
    "        # Renombramos las columnas\n",
    "        df.columns = ['Tipo', 'Test']\n",
    "\n",
    "        # Añadimos la columna Titulo, que será la cabecera del objeto, es decir, el título del caso de prueba, que en DOORS se muestra en negrita\n",
    "        df['Titulo'] = obtener_titulos(df.Tipo, df.Test)\n",
    "\n",
    "        # Añadimos el dataframe al dataframe final, contenedor de todos los casos de prueba\n",
    "        df_resultante = pd.concat([df_resultante, df], ignore_index=True)\n",
    "        l = 0\n",
    "\n",
    "    orden_nuevas_columnas = ['Tipo', 'Titulo', 'Test']\n",
    "    df_resultante = df_resultante[orden_nuevas_columnas]\n",
    "\n",
    "    # Hay casos de pruebas duplicados, donde una entrada hace referencia a una versión anterior a la que se está probando. Las buscamos y eliminamos\n",
    "    df_resultante['Cabecera caso'] = df_resultante['Test'].apply(obtener_cabecera_test)\n",
    "\n",
    "    # Comprobamos aquellos casos donde la cabecera es la misma a excepción de que una indica la versión y otra no\n",
    "    cabeceras_sin_version = [cabecera for cabecera in df_resultante['Cabecera caso'] if '(V2' not in cabecera]\n",
    "\n",
    "    for cabecera in cabeceras_sin_version:\n",
    "        # Buscamos las filas que contienen cabecera, incuyendo la versión a partir de la cual aplica\n",
    "        filas_encontradas = df_resultante.loc[(df_resultante['Test'].str.contains(cabecera + ' (V2', regex=False)) | (df_resultante['Cabecera caso'].str.contains(cabecera + '(V2', regex=False)) | (df_resultante['Cabecera caso'].str.contains(cabecera + '(v2', regex=False)) | (df_resultante['Cabecera caso'].str.contains(cabecera + '(v2', regex=False))]\n",
    "\n",
    "        if len(filas_encontradas) > 0:\n",
    "            # Obtenemos el contenido del test sin la cabecera, que se usará para comprobar que la única diferencia es la versión\n",
    "            texto_test = '\\r\\n'.join(filas_encontradas.loc[filas_encontradas.index[0]]['Test'].split('\\r\\n')[1:])\n",
    "\n",
    "            # Buscamos las filas que contienen la cabecera que estamos buscando, sin la versión, y obtenemos el índice\n",
    "            filas_originales = df_resultante.loc[df_resultante['Test'].str.contains(cabecera + '\\r\\n', regex=False)]\n",
    "            indice = filas_originales.index[0]\n",
    "\n",
    "            # Si, en efecto, se trata de filas idénticas salvo la versión en la cabecera, borramos la entrada anterior\n",
    "            if texto_test in filas_originales.loc[indice]['Test']:\n",
    "                df_resultante.drop(index=indice, inplace=True)\n",
    "\n",
    "\n",
    "    # Eliminamos la columna dado que ya no resulta necesaria\n",
    "    df_resultante.drop(columns=['Cabecera caso'], inplace=True)\n",
    "\n",
    "    # Reordenamos las columnas por el contenido de Test. De esa forma, se facilitará encontrar filas que puedan ser idénticas\n",
    "    df_resultante.sort_values(by=['Titulo', 'Test'], inplace=True)\n",
    "\n",
    "    # Ordenamos por el contenido en test\n",
    "    df_resultante.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Añadimos la columna YAML con un contenido por defecto para facilitar la edición\n",
    "    df_resultante['YAML'] = df_resultante['Tipo'].apply(generar_yaml)\n",
    "\n",
    "    csv_resultante = directorio_exportacion + '..\\\\MODELO.csv'\n",
    "    print(time.strftime('%d/%m/%Y %H:%M:%S') + '\\tGuardando archivo ' + csv_resultante)\n",
    "\n",
    "    intentar_guardar = True\n",
    "    while intentar_guardar:\n",
    "        try:\n",
    "            df_resultante.to_csv(csv_resultante, index=None, sep=';', encoding='windows-1252')\n",
    "            intentar_guardar = False\n",
    "\n",
    "        except PermissionError:\n",
    "            print('¡Error intentando guardar ' + csv_resultante + '! Cierre el fichero para poder guardarlo')\n",
    "            time.sleep(15)\n",
    "\n",
    "obtener_dxl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba1666-976a-4433-b5ac-e6dc0339c899",
   "metadata": {},
   "source": [
    "En este momento se habrán exportado todos los casos de prueba y almacenado como CSV. Antes de continuar, se procederá a comprobar que los datos se extrajeron correctamente.\n",
    "\n",
    "Tras dicha revisión, se procede a realizar un primer estudio de los datos, donde extraeremos la cantidad de palabras distintas y apariciones, cuántos casos de prueba tienen una longitud mayor de 512 caracteres y la longitud mayor encontrada entre los diversos casos de prueba. Todo ello será de utilidad para la elección del modelo pre-entrenado que mejores resultados pueda ofrecer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8022aee-6558-4199-ab3e-938553e31534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de entradas de texto, incluyendo títulos: 1917\n",
      "\n",
      "Palabras diferentes: 4159\n",
      "\n",
      "Palabras distintas, con todas las letras en minúsculas: 3328\n",
      "\n",
      "Entradas con longitud superior a 512 caracteres: 2 \n",
      "Mayor longitud encontrada para texto de entrada: 658 . Índice:  777\n"
     ]
    }
   ],
   "source": [
    "# Abrimos el fichero CSV para su análisos\n",
    "df_resultante = pd.read_csv(os.getcwd() + '\\\\ficheros_excel\\\\MODELO.csv', sep=';', encoding='windows-1252')\n",
    "\n",
    "# Definimos diccionarios, cuyas claves serán las diferentes palabras encontradas y su valor la cantidad de veces que se encontró\n",
    "conteo_palabras = {}\n",
    "conteo_palabras_minusculas = {}\n",
    "\n",
    "longitud_entrada_mas_larga = 0\n",
    "\n",
    "# Se extraen los valores de la columna de Test, pues contiene los textos a analizar\n",
    "valores = df_resultante['Test'].values.tolist()\n",
    "\n",
    "# Se introduce el conteo de palabras en el diccionario\n",
    "i = 0\n",
    "while i < len(valores):\n",
    "    palabras = re.findall(r'(\\b[a-zA-Z0-9]+\\b)', valores[i])\n",
    "    for palabra in palabras:\n",
    "        try:\n",
    "            conteo_palabras[palabra] = conteo_palabras[palabra] + 1\n",
    "            conteo_palabras_minusculas[palabra.lower()] = conteo_palabras_minusculas[palabra.lower()] + 1\n",
    "\n",
    "        except KeyError:\n",
    "            conteo_palabras[palabra] = 1\n",
    "            conteo_palabras_minusculas[palabra.lower()] = 1\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "#valores = df_resultante['Test'].values.tolist()\n",
    "i = 0\n",
    "indice_entrada_mas_larga = 0\n",
    "num_entradas_long_mayor_que_512 = 0\n",
    "while i < len(valores):\n",
    "    entrada_texto = valores[i].split()\n",
    "\n",
    "    # Eliminamos signos de puntuación dado que no cuentan como tokens\n",
    "    while '.' in entrada_texto:\n",
    "        entrada_texto.remove(entrada_texto[entrada_texto.index('.')])\n",
    "\n",
    "    while ',' in entrada_texto:\n",
    "        entrada_texto.remove(entrada_texto[entrada_texto.index(',')])\n",
    "\n",
    "    while ';' in entrada_texto:\n",
    "        entrada_texto.remove(entrada_texto[entrada_texto.index(';')])\n",
    "\n",
    "    while '_' in entrada_texto:\n",
    "        entrada_texto.remove(entrada_texto[entrada_texto.index('_')])\n",
    "\n",
    "    while '-' in entrada_texto:\n",
    "        entrada_texto.remove(entrada_texto[entrada_texto.index('-')])\n",
    "\n",
    "    while '\"' in entrada_texto:\n",
    "        entrada_texto.remove(entrada_texto[entrada_texto.index('\"')])\n",
    "\n",
    "    while '\\'' in entrada_texto:\n",
    "        entrada_texto.remove(entrada_texto[entrada_texto.index('\\'')])\n",
    "\n",
    "    if len(entrada_texto) > longitud_entrada_mas_larga:\n",
    "        longitud_entrada_mas_larga = len(valores[i].split())\n",
    "        indice_entrada_mas_larga = i\n",
    "\n",
    "    if len(entrada_texto) > 512:\n",
    "        num_entradas_long_mayor_que_512 = num_entradas_long_mayor_que_512 + 1\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "print('Cantidad total de entradas de texto, incluyendo títulos:', len(valores))\n",
    "print('\\nPalabras diferentes: ' + str(len(conteo_palabras.keys())))\n",
    "print('\\nPalabras distintas, con todas las letras en minúsculas: ' + str(len(conteo_palabras_minusculas.keys())))\n",
    "print('\\nEntradas con longitud superior a 512 caracteres:', num_entradas_long_mayor_que_512,\n",
    "      '\\nMayor longitud encontrada para texto de entrada:', longitud_entrada_mas_larga, '. Índice: ',\n",
    "      indice_entrada_mas_larga)\n",
    "\n",
    "with open(os.getcwd() + '\\\\ficheros_excel\\\\conteo_datos.csv', mode='w', encoding='windows-1252') as f:\n",
    "    f.write('Palabra;Veces escrita\\n')\n",
    "    for key_palabra in conteo_palabras.keys():\n",
    "        f.write(key_palabra + ';' + str(conteo_palabras[key_palabra]) + '\\n')\n",
    "\n",
    "with open(os.getcwd() + '\\\\ficheros_excel\\\\conteo_datos_minúsculas.csv', mode='w', encoding='windows-1252') as f:\n",
    "    f.write('Palabra;Veces escrita\\n')\n",
    "    for key_palabra in conteo_palabras_minusculas.keys():\n",
    "        f.write(key_palabra + ';' + str(conteo_palabras_minusculas[key_palabra]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b3a85-fae0-4fc9-a2cb-777c36a6a7df",
   "metadata": {},
   "source": [
    "Tras el primer análisis de los datos, procederemos a analizar la página web sobre la que se realizarán pruebas. Para ello, codificaremos funciones que ayudarán en la extracción de xpaths y capturas de imagen asociadas, tanto de forma voraz donde se intentará de forma automática realizar clics y clics derecho de ratón o ponerlo sobre elementos para así obtener el mayor contenido posible como manualmente, donde se indicará el xpath a partir del cual obtener la información y que podrá servir para completar la obtención de todos aquellos elementos que puedan existir.\n",
    "\n",
    "Antes de dichas funciones, se codifica aquella que se encargará de guardar los resultados obtenidos en la obtención de los diversos xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01eafdb9-8208-4ef2-b93c-b39879e47389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_df_como_excel(df, nombre_fichero, aplicar_filtro=True, aplicar_colorizacion=True, contiene_imagenes=False):\n",
    "    intentar_guardar = True\n",
    "\n",
    "    # Creamos una lista vacía que contendrá los DataFrames, pudiendo ser uno o varios en función del número de filas que tenga\n",
    "    # El motivo de esta acción es, además de evitar que supere el tamaño máximo permitido, poder abrir el fichero en excel\n",
    "    dfs = []\n",
    "    if len(df) > 10000:\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "            if i + 10000 < len(df):\n",
    "                dfs.append(df[i:i + 10000])\n",
    "\n",
    "            else:\n",
    "                dfs.append(df[i:])\n",
    "\n",
    "            i = i + 10000\n",
    "\n",
    "    else:\n",
    "        dfs.append(df)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(dfs):\n",
    "        while intentar_guardar:\n",
    "            try:\n",
    "                if len(dfs) > 1:\n",
    "                    excel_writer = pd.ExcelWriter(nombre_fichero.replace('.xlsx', '_' + str(i) + '.xlsx'))\n",
    "\n",
    "                else:\n",
    "                    excel_writer = pd.ExcelWriter(nombre_fichero)\n",
    "\n",
    "                nombre_hoja = nombre_fichero.replace('.xlsx', '')\n",
    "                if '\\\\' in nombre_hoja:\n",
    "                    nombre_hoja = nombre_hoja.split('\\\\')[-1]\n",
    "\n",
    "                dfs[i].to_excel(excel_writer=excel_writer, sheet_name=nombre_hoja, index=None)\n",
    "                workbook = excel_writer.book\n",
    "                worksheet = excel_writer.sheets[nombre_hoja]\n",
    "\n",
    "                # Calculamos el ancho de la columna, que ayudará a poder visualizar el contenido sin tener que modificar en excel\n",
    "                columnas_hoja = dfs[i].columns.tolist()\n",
    "                for nombre_columna in columnas_hoja:\n",
    "                    ancho_columna = len(nombre_columna)\n",
    "                    valores_columna = dfs[i][nombre_columna]\n",
    "\n",
    "                    for texto_celda in valores_columna:\n",
    "                        if type(texto_celda) is str and '\\r\\n' in texto_celda:\n",
    "                            texto_celda = texto_celda.split('\\r\\n')\n",
    "                            long_max_valores_columna = len(max(texto_celda, key=len))\n",
    "\n",
    "                        elif type(texto_celda) is str and '\\n' in texto_celda:\n",
    "                            texto_celda = texto_celda.split('\\n')\n",
    "                            long_max_valores_columna = len(max(texto_celda, key=len))\n",
    "\n",
    "                        elif type(texto_celda) is str:\n",
    "                            long_max_valores_columna = len(texto_celda)\n",
    "\n",
    "                        if long_max_valores_columna > ancho_columna:\n",
    "                            ancho_columna = long_max_valores_columna\n",
    "\n",
    "                        # En caso de que se llegue a una columna a una longitud superior a 150, establecemos la anchura máxima a 150 y pasamos a procesar la siguiente columna\n",
    "                        if ancho_columna > 150:\n",
    "                            ancho_columna = 150\n",
    "                            break\n",
    "\n",
    "                    longitud_columna = ancho_columna + 6\n",
    "                    posicion_columna = columnas_hoja.index(nombre_columna)\n",
    "                    worksheet.set_column(posicion_columna, posicion_columna, longitud_columna)\n",
    "\n",
    "                # Colorear celdas de filas impares si se indica por parámetro. Puede ser útil para mejorar visualización del fichero excel\n",
    "                if aplicar_colorizacion:\n",
    "                    formato_celda = workbook.add_format()\n",
    "                    formato_celda.set_bg_color('c5d9f1')\n",
    "\n",
    "                    j = 1\n",
    "                    rows_indexes = df.index.tolist()\n",
    "                    while j < len(df):\n",
    "                        if j % 2 != 0:\n",
    "                            k = 0\n",
    "                            while k < len(columnas_hoja):\n",
    "                                value_to_modify = df.loc[rows_indexes[j]][columnas_hoja[k]]\n",
    "                                worksheet.write(j + 1, k, value_to_modify, formato_celda)\n",
    "                                k = k + 1\n",
    "\n",
    "                        j = j + 1\n",
    "\n",
    "                # Aplicar filtros si se indica, equivalente a ir a Datos -> Filtro en Excel\n",
    "                if aplicar_filtro:\n",
    "                    worksheet.autofilter(0, 0, len(df), len(columnas_hoja) - 1)\n",
    "\n",
    "                # Sustituimos el nombre del fichero en la columna PNG por el contenido de la imagen en la propia celda\n",
    "                if contiene_imagenes:\n",
    "                    fila = 1\n",
    "                    while fila <= len(df):\n",
    "                        fichero_imagen = df.loc[fila-1]['PNG']\n",
    "                        if fichero_imagen is not None and fichero_imagen != '':\n",
    "                            try:\n",
    "                                worksheet.embed_image(fila, df.columns.tolist().index('PNG'), fichero_imagen)\n",
    "\n",
    "                            except (FileNotFoundError, Exception) as e:\n",
    "                                print(e)\n",
    "                                pass\n",
    "\n",
    "                        fila = fila + 1\n",
    "\n",
    "                # Terminamos de guardar el fichero. Si fuese bien, se establece intentar guardar a False para salir del bucle\n",
    "                excel_writer.close()\n",
    "                intentar_guardar = False\n",
    "\n",
    "            except PermissionError:\n",
    "                print('¡Error, parece que el fichero ' + nombre_fichero + ' se encuentra abierto! Cierre el fichero para poder guardarlo')\n",
    "                time.sleep(10)\n",
    "\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47117c0e-5ebe-41ee-8083-b26cfbb20032",
   "metadata": {},
   "source": [
    "Por otra parte, se muestra a continuación el código utilizado para el acceso al software, inicio y cierre de sesión para poder posteriormente extraer información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec14deb-71bb-4a0f-83c7-e7d49183de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu nombre de usuario:  jmarrieta\n",
      "Introduce tu contraseña:  ········\n",
      "Indique la URL a la que se debe acceder: https://impact-formacion.enaire.es/\n"
     ]
    }
   ],
   "source": [
    "from screeninfo import get_monitors\n",
    "\n",
    "# Dado que el Software implementado está diseñado para pantallas de 4K, obtenemos con ayuda de esta función las pantallas disponibles y su resolución\n",
    "def obtener_info_pantallas():\n",
    "    monitores = get_monitors()\n",
    "\n",
    "    propiedades_monitor = None\n",
    "    ancho_monitor = 0\n",
    "    alto_monitor = 0\n",
    "    i = 0\n",
    "    while i < len(monitores):\n",
    "        if monitores[i].width > ancho_monitor and monitores[i].height > alto_monitor:\n",
    "            propiedades_monitor = monitores[i]\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    return propiedades_monitor\n",
    "\n",
    "# Obtenemos las propiedades del monitor y creamos un objeto de Tests, a cuyo constructor pasamos como parámetro las propiedades del monitor con mayor resolución\n",
    "propiedades_monitor = obtener_info_pantallas()\n",
    "tests = selenium_tests.Tests(propiedades_monitor)\n",
    "\n",
    "# Configuramos el driver\n",
    "tests.configurar_driver()\n",
    "\n",
    "# Iniciamos la sesión\n",
    "tests.iniciar_sesion(params={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aed150-0f64-4954-838b-386f987c8b92",
   "metadata": {},
   "source": [
    "Los siguientes métodos serán los utilizados para la extracción de contenido de la página web sobre la que se realizarán las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba302ada-34d9-4b59-94f3-df7ebb274d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_df_xpaths():\n",
    "    df_xpaths = pd.DataFrame(columns=['XPATH', 'PNG', 'ACCESO DESDE', 'PALABRAS CLAVE', 'CONTENIDO HTML', 'XPATHS_CONTENIDO_HTML', 'REVISADO'])\n",
    "    datos_elementos_no_repetibles = None\n",
    "\n",
    "    # Comprobamos si ya existen datos almacenados para no empezar desde cero innecesariamente\n",
    "    try:\n",
    "        with open('df_xpaths.pickle', mode='rb') as fichero_pickle:\n",
    "            df_xpaths = pickle.load(fichero_pickle)\n",
    "\n",
    "            # Creamos unas listas de elementos cuyos valores y etiquetas o xpaths son cambiantes y bastaría con tener uno a modo de ejemplo\n",
    "            datos_elementos_no_repetibles = ['class=\"app\"', 'hist-cell TRANSPARENT', 'Total flights', 'hist-cell ifr-arr', 'A2SA', 'A1SR4', 'GCCC', 'PREOPS', 'nmOTMVPeriodPeak', 'timeline', 'rowFL']\n",
    "\n",
    "            for dato_no_repetible in datos_elementos_no_repetibles:\n",
    "                existe = df_xpaths['XPATH'].str.contains(dato_no_repetible)\n",
    "                if existe.any() is False:\n",
    "                    datos_elementos_no_repetibles.remove(dato_no_repetible)\n",
    "\n",
    "    # Si se produce un error, se debe a que no existe el fichero. Lo capturamos y no hacemos nada\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    return df_xpaths\n",
    "\n",
    "def obtener_xpath_de_html(contenido_html):\n",
    "    contenido_html = contenido_html.split('<')\n",
    "    xpath_a_devolver = '//' + re.sub(r'data.+\"\"\\s', '', contenido_html[1][:contenido_html[1].index('>')])\n",
    "\n",
    "    xpaths_especiales = ['svg', 'path']\n",
    "    if xpath_a_devolver.split()[0].replace('//', '') in xpaths_especiales:\n",
    "        xpath_a_devolver = xpath_a_devolver.replace(xpath_a_devolver.split()[0], '//*[name()=\"' + xpath_a_devolver.split()[0].replace('//', '') + '\"]')\n",
    "\n",
    "    xpath_a_devolver = re.sub(r'\"\\s', '\"][@', xpath_a_devolver)\n",
    "    xpath_a_devolver = xpath_a_devolver.replace(' ', '[@', 1)\n",
    "    if '[' in xpath_a_devolver:\n",
    "        xpath_a_devolver = xpath_a_devolver + ']'\n",
    "\n",
    "    try:\n",
    "        texto_elemento = contenido_html[1]\n",
    "        texto_elemento = texto_elemento[texto_elemento.index('>') + 1:]\n",
    "        texto_elemento = re.sub(r'^\\s{1,}|\\s{1,}$', '', texto_elemento)\n",
    "        if len(texto_elemento) > 0 and re.search(r'(text\\(\\)\\,\\s\"\\d+:\\d+)', xpath_a_devolver) is None and re.search(r'(text\\(\\),\\s\"\\d+\\/\\d+\\/\\d+)', xpath_a_devolver) is None:\n",
    "            xpath_a_devolver = xpath_a_devolver + '[contains(text(), \"' + texto_elemento + '\")]'\n",
    "\n",
    "    except (IndexError, ValueError):\n",
    "        # Se da el error si es un único tag sin otro a continuación o sin texto: No necesitamos hacer nada, por lo que puede ignorarse\n",
    "        pass\n",
    "\n",
    "    return xpath_a_devolver\n",
    "\n",
    "\n",
    "def obtener_xpaths_de_html(contenido_html):\n",
    "    posibles_xpaths = []\n",
    "\n",
    "    contenido_html_en_lista = contenido_html.split('><')\n",
    "    contenido_html_en_lista = [('<' + contenido) if contenido.startswith('<') is False else contenido for contenido in contenido_html_en_lista]\n",
    "    contenido_html_en_lista = [(contenido + '>') if contenido.endswith('>') is False else contenido for contenido in contenido_html_en_lista]\n",
    "\n",
    "    # Generamos en primer lugar un xpath a partir de cada una de las etiquetas\n",
    "    i = 0\n",
    "    while i < len(contenido_html_en_lista):\n",
    "        if contenido_html_en_lista[i].startswith('<!') is False:\n",
    "            posibles_xpaths.append(obtener_xpath_de_html(contenido_html_en_lista[i]))\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    l = 0\n",
    "    return posibles_xpaths\n",
    "\n",
    "\n",
    "def obtener_xpath_de_elemento(elemento):\n",
    "    excepciones_nombre_tag = ['svg', 'path']\n",
    "    if elemento.tag_name not in excepciones_nombre_tag:\n",
    "        xpath_a_devolver = '//' + elemento.tag_name\n",
    "\n",
    "    else:\n",
    "        xpath_a_devolver = '//*[name()=\"' + elemento.tag_name + '\"]'\n",
    "\n",
    "    atributos = tests.driver.execute_script('var items = {}; for (index = 0; index < arguments[0].attributes.length; ++index) { items[arguments[0].attributes[index].name] = arguments[0].attributes[index].value }; return items;', elemento)\n",
    "    if atributos is not None and len(atributos.keys()) > 0:\n",
    "        # Comprobamos en primer lugar si el elemento tuviese un id, en cuyo caso se toma solo ese atributo dado que es un valor único\n",
    "        if 'id' in atributos:\n",
    "            xpath_a_devolver = xpath_a_devolver + '[@id=\"' + elemento.get_attribute('id') + '\"]'\n",
    "\n",
    "        else:\n",
    "            for atributo in atributos:\n",
    "                if elemento.get_attribute(atributo) is not None and elemento.get_attribute(atributo) != '':\n",
    "                    xpath_a_devolver = xpath_a_devolver + '[@' + atributo + '=\"' + elemento.get_attribute(atributo) + '\"]'\n",
    "\n",
    "    try:\n",
    "        outer_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"', '', elemento.get_attribute('outerHTML'))\n",
    "        texto_elemento = outer_html.split('<')[1]\n",
    "        texto_elemento = texto_elemento[texto_elemento.index('>') + 1:]\n",
    "        texto_elemento = re.sub(r'^\\s{1,}|\\s{1,}$', '', texto_elemento)\n",
    "        if 'Last Update' in elemento.text and '\\n' not in elemento.text:\n",
    "            l = 0\n",
    "\n",
    "        if len(texto_elemento) > 0 and re.search(r'(text\\(\\)\\,\\s\"\\d+:\\d+)', xpath_a_devolver) is None and re.search(r'(text\\(\\),\\s\"\\d+\\/\\d+\\/\\d+)', xpath_a_devolver) is None:\n",
    "            xpath_a_devolver = xpath_a_devolver + '[contains(text(), \"' + texto_elemento + '\")]'\n",
    "\n",
    "    except (IndexError, ValueError):\n",
    "        # Se da el error si es un único tag sin otro a continuación: No necesitamos hacer nada, por lo que puede ignorarse\n",
    "        pass\n",
    "\n",
    "    return xpath_a_devolver\n",
    "\n",
    "\n",
    "def obtener_xpaths_voraz():\n",
    "    datos_elementos_no_repetibles = None\n",
    "\n",
    "    # Obtenemos el dataframe con los xpaths, que estará vacío en caso de ser la primera ejecución\n",
    "    df_xpaths = obtener_df_xpaths()\n",
    "\n",
    "    # Creamos el directorio que contendrá las capturas. De existir el directorio no se hace nada más\n",
    "    os.makedirs(os.getcwd() + '\\\\capturas', exist_ok=True)\n",
    "\n",
    "    # Se espera a detectar que existe el contenedor cuya clase contiene el texto tab-ws\n",
    "    params = {'esperar': {'xpath': '//div[contains(@class,\"tab-ws\")]'}, 'tipo_espera': 'presencia'}\n",
    "    tests.esperar_elemento(params)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Preguntamos al usuario la URL de acceso\n",
    "    url_acceso = input('Indique la URL a la que se debe acceder:' )\n",
    "    tests.driver.get(url_acceso)\n",
    "\n",
    "    # En caso de estar actualizándose los datos, esperamos a que finalice\n",
    "    params = {'esperar': {'xpath': '//div[@class=\"v-card__text\"][contains(text(), \"Please stand by\")]'}, 'tipo_espera': 'invisibilidad'}\n",
    "    tests.esperar_elemento(params)\n",
    "\n",
    "    # Guardamos el contenido HTML, que nos servirá posteriormente para detectar otros posibles xpaths\n",
    "    # Sustituimos valores de fecha y hora, ya que pueda hacer que parezca que dos etiquetas son distintas cuando la diferencia es la hora actial\n",
    "    contenido_html = re.sub(r'\\d+:\\d+|\\d+\\/\\d+\\/\\d+', '', tests.driver.page_source)\n",
    "    contenido_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', contenido_html)\n",
    "\n",
    "    try:\n",
    "        if len(df_xpaths) == 0:\n",
    "            # Buscamos todos los elementos que haya en la página\n",
    "            elementos_encontrados = tests.driver.find_elements(By.XPATH, '//body//*')\n",
    "\n",
    "            tags_a_eliminar = ['html', 'link', 'title', 'style', 'script', 'head', 'meta']\n",
    "            elementos_encontrados = [elemento for elemento in elementos_encontrados if elemento is not None and elemento.tag_name is not None and elemento.tag_name not in tags_a_eliminar]\n",
    "\n",
    "            for elemento in tqdm(elementos_encontrados, unit='xpath', total=len(elementos_encontrados)):\n",
    "                try:\n",
    "                    xpath_a_aniadir = obtener_xpath_de_elemento(elemento)\n",
    "\n",
    "                    if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():\n",
    "                        try:\n",
    "                            contenido_html_sin_separar = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elemento.get_attribute('outerHTML'))\n",
    "                            xpaths_contenido_html = obtener_xpaths_de_html(contenido_html_sin_separar)\n",
    "\n",
    "                            # Obtenemos las palabras clave, que serán de utilidad para poder buscar los xpaths necesarios\n",
    "                            palabras_clave = re.findall(r'\\b\\w+\\b', xpath_a_aniadir)\n",
    "                            palabras_clave.append(re.sub(r'\\d+\\/\\d+\\/\\d+|\\d{2}:\\d{2}', '', elemento.text).replace('\\n\\n', ',').replace('\\n', ','))\n",
    "\n",
    "                            j = 10\n",
    "                            while j < len(palabras_clave):\n",
    "                                palabras_clave.insert(j, '\\n')\n",
    "\n",
    "                                j = j + 10\n",
    "\n",
    "                            # Eliminamos los valores repetidos, ya que en este caso no es necesario tener valores duplicados\n",
    "                            palabras_clave = list(set(palabras_clave))\n",
    "                            palabras_clave = ','.join(palabras_clave)\n",
    "\n",
    "                            # Guardamos la imagen en un fichero, cuyo nombre será el del índice si se hizo captura\n",
    "                            if type(elemento.screenshot_as_png) is bytes:\n",
    "                                nombre_fichero_imagen = os.getcwd() + '\\\\capturas\\\\captura' + str(len(df_xpaths)) + '.png'\n",
    "                                elemento.screenshot(nombre_fichero_imagen)\n",
    "                                df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, nombre_fichero_imagen, '', palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                            else:\n",
    "                                df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', '',  palabras_clave, contenido_html_sin_separar,  '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                        except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                            df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', '',  palabras_clave, contenido_html_sin_separar, '', '']\n",
    "\n",
    "                except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                    pass\n",
    "\n",
    "        # Convertimos contenido_html a lista, pues posteriormente se utilizará para comprobar si acciones como un clic hace que se muestre nuevo contenido\n",
    "        contenido_html = contenido_html.split('><')\n",
    "        contenido_html = [('<' + contenido) if contenido.startswith('<') is False else contenido for contenido in contenido_html]\n",
    "        contenido_html = [(contenido + '>') if contenido.endswith('>') is False else contenido for contenido in contenido_html]\n",
    "\n",
    "        acciones = ['clic_derecho', 'hover', 'clic']\n",
    "\n",
    "        # Ya obtenidos los xpath de la web, ejecutamos las acciones de clic, clic derecho y hover y buscamos posibles xpaths a añadir\n",
    "        barra_progreso = tqdm(total=len(df_xpaths), unit='xpath(s)')\n",
    "        i = 0\n",
    "        while i < len(df_xpaths):\n",
    "            if df_xpaths.loc[i]['REVISADO'] != 'SI':\n",
    "                for accion in acciones:\n",
    "                    if accion == acciones[-1]:\n",
    "                        # Lo marcamos como revisado para, en caso de retomar el proceso, no volver a hacer lo ya realizado\n",
    "                        df_xpaths.loc[i]['REVISADO'] = 'SI'\n",
    "\n",
    "                    try:\n",
    "                        # En caso de estar actualizándose los datos, esperamos a que finalice\n",
    "                        params = {'esperar': {'xpath': '//div[@class=\"v-card__text\"][contains(text(), \"Please stand by\")]'}, 'tipo_espera': 'invisibilidad'}\n",
    "                        tests.esperar_elemento(params)\n",
    "\n",
    "                        match accion:\n",
    "                            case 'clic':\n",
    "                                # Hacemos clic sobre el elemento\n",
    "                                tests.hacer_clic({'xpath': df_xpaths.loc[i]['XPATH']})\n",
    "\n",
    "                            case 'clic_derecho':\n",
    "                                # Hacemos clic derecho sobre el elemento\n",
    "                                tests.hacer_clic_derecho({'xpath': df_xpaths.loc[i]['XPATH']})\n",
    "\n",
    "                            case 'hover':\n",
    "                                 # Intentamos hacer hover sobre el elemento\n",
    "                                 tests.hacer_hover({'xpath': df_xpaths.loc[i]['XPATH']})\n",
    "\n",
    "                        # Obtenemos el nuevo contenido HTML eliminando datos de fecha y/o hora y comparamos con el almacenado para ver si es diferente\n",
    "                        # En primer lugar, eliminamos etiquetas de comentarios y datos de fecha y hora que puedan hacer que hay diferencias que para análisis no deberían existir en realidad\n",
    "                        nuevo_contenido_html = re.sub(r'\\d+:\\d+|\\d+\\/\\d+\\/\\d+', '', tests.driver.page_source).replace('<!---->', '')\n",
    "                        nuevo_contenido_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', nuevo_contenido_html)\n",
    "                        nuevo_contenido_html = nuevo_contenido_html.split('><')\n",
    "                        nuevo_contenido_html = [('<' + contenido) if contenido.startswith('<') is False else contenido for contenido in nuevo_contenido_html]\n",
    "                        nuevo_contenido_html = [(contenido + '>') if contenido.endswith('>') is False else contenido for contenido in nuevo_contenido_html]\n",
    "\n",
    "                        # Eliminamos aquello que no sea diferente o nuevo\n",
    "                        nuevo_contenido_html = [contenido for contenido in nuevo_contenido_html if contenido not in contenido_html]\n",
    "\n",
    "                        # Eliminamos aquellos elementos que puedan considerarse repetidos\n",
    "                        j = 0\n",
    "                        while j < len(datos_elementos_no_repetibles):\n",
    "                            k = 0\n",
    "                            while k < len(nuevo_contenido_html):\n",
    "                                if nuevo_contenido_html[k] in datos_elementos_no_repetibles[j]:\n",
    "                                    datos_elementos_no_repetibles.remove(datos_elementos_no_repetibles[j])\n",
    "\n",
    "                                else:\n",
    "                                    k = k + 1\n",
    "\n",
    "                            j = j + 1\n",
    "\n",
    "                        for contenido in nuevo_contenido_html:\n",
    "                            xpath_a_aniadir = obtener_xpath_de_html(contenido)\n",
    "\n",
    "                            if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():\n",
    "                                try:\n",
    "                                    # Buscamos todos los elementos que haya en la página\n",
    "                                    elementos_encontrados = tests.driver.find_elements(By.XPATH, xpath_a_aniadir + '//*')\n",
    "                                    elementos_encontrados = [elemento for elemento in elementos_encontrados if elemento.tag_name not in tags_a_eliminar]\n",
    "\n",
    "                                    for elemento in elementos_encontrados:\n",
    "                                        try:\n",
    "                                            xpath_a_aniadir = obtener_xpath_de_elemento(elemento)\n",
    "\n",
    "                                            if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():# and '[' in xpath_a_aniadir:\n",
    "                                                try:\n",
    "                                                    contenido_html_sin_separar = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elemento.get_attribute('outerHTML'))\n",
    "                                                    xpaths_contenido_html = obtener_xpaths_de_html(contenido_html_sin_separar)\n",
    "\n",
    "                                                    # Obtenemos las palabras clave, que serán de utilidad para poder buscar los xpaths necesarios\n",
    "                                                    palabras_clave = re.findall(r'\\b\\w+\\b', xpath_a_aniadir)\n",
    "                                                    palabras_clave.append(re.sub(r'\\d+\\/\\d+\\/\\d+|\\d{2}:\\d{2}', '', elemento.text).replace('\\n\\n', ',').replace('\\n', ','))\n",
    "\n",
    "                                                    j = 10\n",
    "                                                    while j < len(palabras_clave):\n",
    "                                                        palabras_clave.insert(j, '\\n')\n",
    "\n",
    "                                                        j = j + 10\n",
    "\n",
    "                                                    # Eliminamos los valores repetidos, ya que en este caso no es necesario tener valores duplicados\n",
    "                                                    palabras_clave = list(set(palabras_clave))\n",
    "                                                    palabras_clave = ','.join(palabras_clave)\n",
    "\n",
    "                                                    # Si el botón de Cancelar, Cerrar o No está presente y estamos en la acción de clic, haremos clic sobre él para poder continuar\n",
    "                                                    if accion == 'clic' and ('CANCEL' in xpath_a_aniadir.upper()) or ('CLOSE' in xpath_a_aniadir.upper()) or ('NO' in xpath_a_aniadir.upper()):\n",
    "                                                        tests.hacer_clic({'xpath': xpath_a_aniadir})\n",
    "\n",
    "                                                    # Guardamos la imagen en un fichero, cuyo nombre será el del índice si se hizo captura\n",
    "                                                    if type(elemento.screenshot_as_png) is bytes:\n",
    "                                                        nombre_fichero_imagen = os.getcwd() + '\\\\capturas\\\\captura' + str(len(df_xpaths)) + '.png'\n",
    "                                                        df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, nombre_fichero_imagen, df_xpaths.loc[i]['XPATH'], palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                                                        # Guardamos la imagen en un fichero, cuyo nombre será el del índice\n",
    "                                                        elemento.screenshot(nombre_fichero_imagen)\n",
    "\n",
    "                                                except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                                                    df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', df_xpaths.loc[i]['XPATH'], palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                                        except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException, selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                                            pass\n",
    "\n",
    "                                except (selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.InvalidSelectorException, selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                                    pass\n",
    "\n",
    "                    except (AttributeError, selenium.common.exceptions.ElementNotInteractableException,selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                        pass\n",
    "\n",
    "                    # En caso de que suceda algún error no contemplado, lo capturamos para poder guardar todo lo que se haya encontrado\n",
    "                    except Exception:\n",
    "                        # Guardamos fichero pickle para, en caso de volver a ejecutar, poder hacerlo desde donde nos hubiésemos quedado\n",
    "                        with open('df_xpaths.pickle', mode='wb') as fichero_pickle:\n",
    "                            pickle.dump(df_xpaths, fichero_pickle)\n",
    "\n",
    "                        # Acciones como CTRL+C provocan este error. Lo capturamos para poder guardar lo hecho y por si se quiere revisar durante el proceso que va por buen camino\n",
    "                        print('Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido')\n",
    "                        guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)\n",
    "\n",
    "            i = i + 1\n",
    "            barra_progreso.total = len(df_xpaths)\n",
    "            barra_progreso.update(1)\n",
    "\n",
    "        barra_progreso.close()\n",
    "\n",
    "        # Guardamos el fichero Excel\n",
    "        guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Guardamos fichero pickle para, en caso de volver a ejecutar, poder hacerlo desde donde nos hubiésemos quedado\n",
    "        with open('df_xpaths.pickle', mode='wb') as fichero_pickle:\n",
    "            pickle.dump(df_xpaths, fichero_pickle)\n",
    "\n",
    "        # Acciones como CTRL+C provocan este error. Lo capturamos para poder guardar lo hecho y por si se quiere revisar durante el proceso que va por buen camino\n",
    "        print('Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido')\n",
    "        guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)\n",
    "\n",
    "\n",
    "def obtener_xpaths_asistido():\n",
    "    datos_elementos_no_repetibles = None\n",
    "\n",
    "    # Obtenemos el dataframe con los xpaths, que estará vacío en caso de ser la primera ejecución\n",
    "    df_xpaths = obtener_df_xpaths()\n",
    "\n",
    "    # Creamos el directorio que contendrá las capturas. De ya existir, no se hace nada\n",
    "    os.makedirs(os.getcwd() + '\\\\capturas', exist_ok=True)\n",
    "\n",
    "    params = {'esperar': {'xpath': '//div[contains(@class,\"tab-ws\")]'}, 'tipo_espera': 'presencia'}\n",
    "    tests.esperar_elemento(params)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Preguntamos al usuario la URL de acceso\n",
    "    url_acceso = input('Indique la URL a la que se debe acceder:' )\n",
    "    tests.driver.get(url_acceso)\n",
    "\n",
    "    # En caso de estar actualizándose los datos, esperamos a que finalice\n",
    "    params = {'esperar': {'xpath': '//div[@class=\"v-card__text\"][contains(text(), \"Please stand by\")]'}, 'tipo_espera': 'invisibilidad'}\n",
    "    tests.esperar_elemento(params)\n",
    "\n",
    "    # Guardamos el contenido HTML, que nos servirá posteriormente para detectar otros posibles xpaths\n",
    "    # Sustituimos valores de fecha y hora, ya que podría hacer que parezca que dos etiquetas son distintas cuando la diferencia es la hora actual\n",
    "    contenido_html = re.sub(r'\\d+:\\d+|\\d+\\/\\d+\\/\\d+', '', tests.driver.page_source)\n",
    "    contenido_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', contenido_html)\n",
    "\n",
    "    salir = False\n",
    "    while salir is False:\n",
    "        try:\n",
    "            # Preguntamos al usuario por el xpath a buscar\n",
    "            xpath_a_buscar = input('Introduzca el xpath a buscar, o escriba salir para finalizar: ')\n",
    "            \n",
    "            if 'salir' in xpath_a_buscar:\n",
    "                salir = True\n",
    "                break\n",
    "            \n",
    "            # Si el xpath indicado no acaba en //*, se lo añadimos para que busque el contenido dentro del tag\n",
    "            if xpath_a_buscar.endswith('//*') is False:\n",
    "                xpath_a_buscar = xpath_a_buscar + '//*'\n",
    "            \n",
    "            # Buscamos todos los elementos que haya en la página a partir del xpath dado\n",
    "            elementos_encontrados = tests.driver.find_elements(By.XPATH, xpath_a_buscar)\n",
    "\n",
    "            tags_a_eliminar = ['html', 'link', 'title', 'style', 'script', 'head', 'meta']\n",
    "            elementos_encontrados = [elemento for elemento in elementos_encontrados if elemento.tag_name not in tags_a_eliminar]\n",
    "\n",
    "            for elemento in tqdm(elementos_encontrados, unit='xpath', total=len(elementos_encontrados)):\n",
    "                try:\n",
    "                    xpath_a_aniadir = obtener_xpath_de_elemento(elemento)\n",
    "\n",
    "                    if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():\n",
    "                        try:\n",
    "                            contenido_html_sin_separar = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elemento.get_attribute('outerHTML'))\n",
    "                            xpaths_contenido_html = obtener_xpaths_de_html(contenido_html_sin_separar)\n",
    "\n",
    "                            # Obtenemos las palabras clave, que serán de utilidad para poder buscar los xpaths necesarios\n",
    "                            palabras_clave = re.findall(r'\\b\\w+\\b', xpath_a_aniadir)\n",
    "                            palabras_clave.append(re.sub(r'\\d+\\/\\d+\\/\\d+|\\d{2}:\\d{2}', '', elemento.text).replace('\\n\\n', ',').replace('\\n', ','))\n",
    "\n",
    "                            j = 10\n",
    "                            while j < len(palabras_clave):\n",
    "                                palabras_clave.insert(j, '\\n')\n",
    "\n",
    "                                j = j + 10\n",
    "\n",
    "                            # Eliminamos los valores repetidos, ya que en este caso no es necesario tener valores duplicados\n",
    "                            palabras_clave = list(set(palabras_clave))\n",
    "                            palabras_clave = ','.join(palabras_clave)\n",
    "\n",
    "                            # Guardamos la imagen en un fichero, cuyo nombre será el del índice si se hizo captura\n",
    "                            if type(elemento.screenshot_as_png) is bytes:\n",
    "                                nombre_fichero_imagen = os.getcwd() + '\\\\capturas\\\\captura' + str(len(df_xpaths)) + '.png'\n",
    "                                elemento.screenshot(nombre_fichero_imagen)\n",
    "                                df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, nombre_fichero_imagen, xpath_a_buscar, palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                            else:\n",
    "                                df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', xpath_a_buscar,  palabras_clave, contenido_html_sin_separar,  '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                        except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                            df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', xpath_a_buscar,  palabras_clave, contenido_html_sin_separar, '', '']\n",
    "\n",
    "                except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                    pass\n",
    "\n",
    "            # Convertimos contenido_html a lista, pues posteriormente se utilizará para comprobar si acciones como un clic hace que se muestre nuevo contenido\n",
    "            contenido_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elementos_encontrados[0].get_attribute('outerHTML'))\n",
    "            contenido_html = contenido_html.split('><')\n",
    "            contenido_html = [('<' + contenido) if contenido.startswith('<') is False else contenido for contenido in contenido_html]\n",
    "            contenido_html = [(contenido + '>') if contenido.endswith('>') is False else contenido for contenido in contenido_html]\n",
    "\n",
    "            # Eliminamos aquellos elementos que puedan considerarse repetidos\n",
    "            j = 0\n",
    "            while j < len(datos_elementos_no_repetibles):\n",
    "                k = 0\n",
    "                while k < len(contenido_html):\n",
    "                    if contenido_html[k] in datos_elementos_no_repetibles[j]:\n",
    "                        datos_elementos_no_repetibles.remove(datos_elementos_no_repetibles[j])\n",
    "\n",
    "                    else:\n",
    "                        k = k + 1\n",
    "\n",
    "                j = j + 1\n",
    "\n",
    "            for contenido in contenido_html:\n",
    "                xpath_a_aniadir = obtener_xpath_de_html(contenido)\n",
    "\n",
    "                if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():\n",
    "                    try:\n",
    "                        # Buscamos todos los elementos que haya en la página\n",
    "                        elementos_encontrados = tests.driver.find_elements(By.XPATH, xpath_a_aniadir + '//*')\n",
    "                        elementos_encontrados = [elemento for elemento in elementos_encontrados if elemento.tag_name not in tags_a_eliminar]\n",
    "\n",
    "                        for elemento in elementos_encontrados:\n",
    "                            try:\n",
    "                                xpath_a_aniadir = obtener_xpath_de_elemento(elemento)\n",
    "\n",
    "                                if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():# and '[' in xpath_a_aniadir:\n",
    "                                    try:\n",
    "                                        contenido_html_sin_separar = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elemento.get_attribute('outerHTML'))\n",
    "                                        xpaths_contenido_html = obtener_xpaths_de_html(contenido_html_sin_separar)\n",
    "\n",
    "                                        # Obtenemos las palabras clave, que serán de utilidad para poder buscar los xpaths necesarios\n",
    "                                        palabras_clave = re.findall(r'\\b\\w+\\b', xpath_a_aniadir)\n",
    "                                        palabras_clave.append(re.sub(r'\\d+\\/\\d+\\/\\d+|\\d{2}:\\d{2}', '', elemento.text).replace('\\n\\n', ',').replace('\\n', ','))\n",
    "\n",
    "                                        j = 10\n",
    "                                        while j < len(palabras_clave):\n",
    "                                            palabras_clave.insert(j, '\\n')\n",
    "\n",
    "                                            j = j + 10\n",
    "\n",
    "                                        # Eliminamos los valores repetidos, ya que en este caso no es necesario tener valores duplicados\n",
    "                                        palabras_clave = list(set(palabras_clave))\n",
    "                                        palabras_clave = ','.join(palabras_clave)\n",
    "\n",
    "                                        # Guardamos la imagen en un fichero, cuyo nombre será el del índice si se hizo captura\n",
    "                                        if type(elemento.screenshot_as_png) is bytes:\n",
    "                                            nombre_fichero_imagen = os.getcwd() + '\\\\capturas\\\\captura' + str(len(df_xpaths)) + '.png'\n",
    "                                            df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, nombre_fichero_imagen, '', palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                                            # Guardamos la imagen en un fichero, cuyo nombre será el del índice\n",
    "                                            elemento.screenshot(nombre_fichero_imagen)\n",
    "\n",
    "                                    except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                                        df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', '', palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                            except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException, selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                                pass\n",
    "\n",
    "                    except (selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.InvalidSelectorException, selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                        pass\n",
    "\n",
    "            # Guardamos el fichero Excel\n",
    "            guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # Guardamos fichero pickle para, en caso de volver a ejecutar, poder hacerlo desde donde nos hubiésemos quedado\n",
    "            with open('df_xpaths.pickle', mode='wb') as fichero_pickle:\n",
    "                pickle.dump(df_xpaths, fichero_pickle)\n",
    "\n",
    "            # Acciones como CTRL+C provocan este error. Se captura para poder guardar la información obtenida\n",
    "            print('Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido')\n",
    "            guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b78cb-5403-4c2a-8ee9-f9d17c26da03",
   "metadata": {},
   "source": [
    "En la siguiente celda se podrá elegir el método de obtención de xpaths deseados. Una vez elegido, se procederá a la extracción hasta su finalización, ya sea por haber terminado el proceso en el caso del método voraz o por finalización por parte del usuario para cualquiera de los métodos de extracción de información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0980299c-05b2-4c41-b087-2f8bff1a8f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Indique el tipo de obtención de XPATHS deseado:\n",
      "1) Voraz\n",
      "2) Asistido\n",
      " Opción:  1\n",
      "Indique la URL a la que se debe acceder: https://impact-formacion.enaire.es/#/main/master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                      | 0/82 [00:00<?, ?xpath(s)/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido\n",
      "Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|█▎                                                                                                            | 1/82 [00:12<17:11, 12.74s/xpath(s)]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido\n",
      "Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▎                                                                                                            | 1/82 [00:23<31:38, 23.44s/xpath(s)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ElementNotInteractableException",
     "evalue": "Message: element not interactable\n  (Session info: chrome=138.0.7204.185)\nStacktrace:\n\tGetHandleVerifier [0x0x7ff6c88ae415+77285]\n\tGetHandleVerifier [0x0x7ff6c88ae470+77376]\n\t(No symbol) [0x0x7ff6c867989c]\n\t(No symbol) [0x0x7ff6c86d1904]\n\t(No symbol) [0x0x7ff6c86c3458]\n\t(No symbol) [0x0x7ff6c86f860a]\n\t(No symbol) [0x0x7ff6c86c2d06]\n\t(No symbol) [0x0x7ff6c86f8820]\n\t(No symbol) [0x0x7ff6c872087f]\n\t(No symbol) [0x0x7ff6c86f83e3]\n\t(No symbol) [0x0x7ff6c86c1521]\n\t(No symbol) [0x0x7ff6c86c22b3]\n\tGetHandleVerifier [0x0x7ff6c8b91efd+3107021]\n\tGetHandleVerifier [0x0x7ff6c8b8c29d+3083373]\n\tGetHandleVerifier [0x0x7ff6c8babedd+3213485]\n\tGetHandleVerifier [0x0x7ff6c88c884e+184862]\n\tGetHandleVerifier [0x0x7ff6c88d055f+216879]\n\tGetHandleVerifier [0x0x7ff6c88b7084+113236]\n\tGetHandleVerifier [0x0x7ff6c88b7239+113673]\n\tGetHandleVerifier [0x0x7ff6c889e298+11368]\n\tBaseThreadInitThunk [0x0x7ffafe12e8d7+23]\n\tRtlUserThreadStart [0x0x7ffb0039c34c+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementNotInteractableException\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m         obtener_xpaths_asistido()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Cerramos la sesión\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcerrar_sesion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:711\u001b[0m, in \u001b[0;36mcerrar_sesion\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:119\u001b[0m, in \u001b[0;36mWebElement.click\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclick\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clicks the element.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    >>> element.click()\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLICK_ELEMENT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:572\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    570\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    571\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mElementNotInteractableException\u001b[0m: Message: element not interactable\n  (Session info: chrome=138.0.7204.185)\nStacktrace:\n\tGetHandleVerifier [0x0x7ff6c88ae415+77285]\n\tGetHandleVerifier [0x0x7ff6c88ae470+77376]\n\t(No symbol) [0x0x7ff6c867989c]\n\t(No symbol) [0x0x7ff6c86d1904]\n\t(No symbol) [0x0x7ff6c86c3458]\n\t(No symbol) [0x0x7ff6c86f860a]\n\t(No symbol) [0x0x7ff6c86c2d06]\n\t(No symbol) [0x0x7ff6c86f8820]\n\t(No symbol) [0x0x7ff6c872087f]\n\t(No symbol) [0x0x7ff6c86f83e3]\n\t(No symbol) [0x0x7ff6c86c1521]\n\t(No symbol) [0x0x7ff6c86c22b3]\n\tGetHandleVerifier [0x0x7ff6c8b91efd+3107021]\n\tGetHandleVerifier [0x0x7ff6c8b8c29d+3083373]\n\tGetHandleVerifier [0x0x7ff6c8babedd+3213485]\n\tGetHandleVerifier [0x0x7ff6c88c884e+184862]\n\tGetHandleVerifier [0x0x7ff6c88d055f+216879]\n\tGetHandleVerifier [0x0x7ff6c88b7084+113236]\n\tGetHandleVerifier [0x0x7ff6c88b7239+113673]\n\tGetHandleVerifier [0x0x7ff6c889e298+11368]\n\tBaseThreadInitThunk [0x0x7ffafe12e8d7+23]\n\tRtlUserThreadStart [0x0x7ffb0039c34c+44]\n"
     ]
    }
   ],
   "source": [
    "tipo_obtencion = input('Indique el tipo de obtención de XPATHS deseado:\\n1) Voraz\\n2) Asistido\\n Opción: ')\n",
    "\n",
    "match tipo_obtencion:\n",
    "    case '1':\n",
    "        obtener_xpaths_voraz()\n",
    "\n",
    "    case '2':\n",
    "        obtener_xpaths_asistido()\n",
    "\n",
    "# Cerramos la sesión\n",
    "tests.cerrar_sesion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db56e38-0e76-4d1c-998d-55485de482c4",
   "metadata": {},
   "source": [
    "A continuación se abrirá el CSV que contiene todos los casos de prueba, sobre el que se añadirán las columnas de posibles índices de datos y sugerencias. Esto busca la facilitación de la redacción de las clases o valores de salida, es decir, la creación del YAML correspondiente a cada caso de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a7ba954-b786-4a2e-8ec6-a6970470cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_posiciones_datos_y_sugerencias():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "    \n",
    "    # Abrimos el fichero Excel que contendrá los xpaths asociados, pues a través de la columna de palabras clave comprobaremos si se puede corresponder a un caso de prueba\n",
    "    df_xpaths = pd.read_excel(io=directorio_ficheros_excel + 'xpaths.xlsx', sheet_name='xpaths', index_col=None)\n",
    "\n",
    "    # Detectamos la codificación del fichero, para así evitar problemas de acentos y otros caracteres especiales\n",
    "    codificacion_detectada = None\n",
    "    with open(directorio_ficheros_excel + 'MODELO.csv', mode='rb') as f:\n",
    "        contenido_fichero = f.read()\n",
    "        codificacion_detectada = chardet.detect(contenido_fichero)['encoding']\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # Una vez detectada, abrimos el CSV que contiene nuestro modelo\n",
    "    df_modelo = pd.read_csv(directorio_ficheros_excel + 'MODELO.csv', sep=';', encoding=codificacion_detectada, index_col=None)\n",
    "    \n",
    "    # Añadimos una columna en el DataFrame del modelo que indicará posibles filas en que se encuentren el/los xpaths que buscamos para el caso de prueba\n",
    "    # Añadiremos además otra con sugerencias\n",
    "    df_modelo['POSIBLES_INDICES_DATOS'] = [''] * len(df_modelo)\n",
    "    df_modelo['SUGERENCIAS'] = [''] * len(df_modelo)\n",
    "\n",
    "    # Creamos una lista que contiene palabras a eliminar del listado de palabras del texto de entrada\n",
    "    palabras_eliminables = ['DATOS', 'Y', 'O', 'EN', 'PARA', 'CALIDAD', 'DE', 'PIE', 'CABECERA', 'MOSTRADOS']\n",
    "\n",
    "    # Creamos a continuación una lista con palabras a partir de las cuales crear las sugerencias\n",
    "    sugerencias = [\n",
    "        'pulsar|Pulsar|clic|seleccionar|Seleccionar',\n",
    "        'tooltip|tool-tip|hover',\n",
    "        'aparece|contiene|Constitución|constitucion',\n",
    "        ' habilitado|deshabilitado'\n",
    "        ]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df_modelo):\n",
    "        # Tanto las sugerencias como la búsqueda de posibles índices que contengan los datos se harán para aquellas filas donde el valor de la columna Tipo es Case\n",
    "        if df_modelo.loc[i]['Tipo'] == 'Case' and df_modelo.loc[i]['YAML'] is not np.NaN:\n",
    "            # Buscamos en primer lugar los posibles índices en que pueda encontrarse lo que nos interesa\n",
    "            palabras = list(set(re.findall(r'\\b[\\w|0-9|\\.|\\\\p{L}\\\\p{M}\\\\p{N}]+\\b|\".+\"', df_modelo.loc[i]['Test'])))\n",
    "            palabras = [re.sub(r'\\(|\\)', '', palabra) for palabra in palabras if palabra.upper() not in palabras_eliminables]\n",
    "\n",
    "            indices_xpaths_encontrados = df_xpaths.loc[df_xpaths['PALABRAS CLAVE'].str.contains('|'.join(palabras))].index.astype('str').tolist()\n",
    "            if len(indices_xpaths_encontrados) > 0:\n",
    "\n",
    "                # En caso de que el número de índices encontrados sea mayor a 30, insertamos retorno de carro cada 30 índices para una mejora de la visualización del fichero final\n",
    "                if len(indices_xpaths_encontrados) > 30:\n",
    "                    indices_xpaths_encontrados = ','.join(indices_xpaths_encontrados).split(',')\n",
    "                    j = 30\n",
    "                    while j < len(indices_xpaths_encontrados):\n",
    "                        indices_xpaths_encontrados.insert(j, '\\r\\n')\n",
    "\n",
    "                        j = j + 30\n",
    "\n",
    "                df_modelo.at[i, 'POSIBLES_INDICES_DATOS'] = ','.join(indices_xpaths_encontrados)\n",
    "\n",
    "            # Buscamos ahora rellenar la columna de sugerencias que puedan facilitar la tarea. Para ello, crearemos una lista con tuplas como contenido\n",
    "            sugerencias_test = []\n",
    "            texto_test = df_modelo.loc[i]['Test']\n",
    "\n",
    "            for sugerencia in sugerencias:\n",
    "                resultado_busqueda = re.search(sugerencia, texto_test)\n",
    "                if resultado_busqueda is not None:\n",
    "                    match sugerencia:\n",
    "                        case 'pulsar|Pulsar|clic|seleccionar|Seleccionar':\n",
    "                            if 'botón' in texto_test or 'icono' in texto_test:\n",
    "                                if 'refresco' in texto_test or 'refrescar' in texto_test or 'actualizar' in texto_test:\n",
    "                                    sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //*[name()=\"path\"][contains(@d, \"M19,8L15\")]')\n",
    "\n",
    "                                elif 'configuración' in texto_test:\n",
    "                                    sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //*[name()=\"path\"][contains(@d, \"M12,15.5A3\")]')\n",
    "\n",
    "                                elif 'añadir' in texto_test:\n",
    "                                    sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //*[name()=\"path\"][contains(@d, \"M19,13H13\")]')\n",
    "\n",
    "                                elif 'papelera' in texto_test or 'borrar' in texto_test:\n",
    "                                    sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //*[name()=\"path\"][contains(@d, \"M9,3V4H4\")]')\n",
    "\n",
    "                                else:\n",
    "                                    resultado_busqueda = re.search(r'botón(\"\\w+\"|\\(.+\\))', texto_test)\n",
    "                                    if resultado_busqueda is not None:\n",
    "                                        resultado_busqueda = resultado_busqueda.group(0)\n",
    "                                        resultado_busqueda = resultado_busqueda.replace('\"', '').replace('()', '').replace(')', '')\n",
    "                                        sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + resultado_busqueda + '\")]')\n",
    "\n",
    "                        case 'tooltip|tool-tip|hover':\n",
    "                            if '• ' in texto_test:\n",
    "                                texto_aniadible = '  - funcion: comprobar_contenido_hover\\r\\n    params:\\r\\n      - xpath: \\r\\n        esperar: \\r\\n          - xpath: \\r\\n        tipo_comprobacion: y\\r\\n        contenido: '\n",
    "                                if '\\r\\n' in texto_test:\n",
    "                                    lineas_texto = texto_test.split('\\r\\n')\n",
    "\n",
    "                                else:\n",
    "                                    lineas_texto = texto_test.split('\\n')\n",
    "\n",
    "                                j = 0\n",
    "                                while j < len(lineas_texto):\n",
    "                                    if '•' in lineas_texto[j] or '-' in lineas_texto[j]:\n",
    "                                        texto_a_aniadir = lineas_texto[j].replace('•', '').replace('-', '')\n",
    "                                        while texto_a_aniadir.startswith(' '):\n",
    "                                            texto_a_aniadir = texto_a_aniadir[1:]\n",
    "\n",
    "                                        texto_aniadible = texto_aniadible + '\\r\\n          - xpath: //*[contains(text(), \"' + texto_a_aniadir + '\")]'\n",
    "\n",
    "                                        j = j + 1\n",
    "\n",
    "                                    else:\n",
    "                                        lineas_texto.remove(lineas_texto[j])\n",
    "\n",
    "                                sugerencias_test.append(texto_aniadible)\n",
    "\n",
    "                            elif 'mensaje:' in texto_test or 'texto:' in texto_test or 'descripción:' in texto_test:\n",
    "                                resultado_busqueda = re.search(r'(?:mensaje\\:|texto\\:|descripción\\:).+\\.{0,1}$', texto_test)\n",
    "                                if resultado_busqueda is not None:\n",
    "                                    resultado_busqueda = resultado_busqueda.group(0).replace('mensaje: ', '').replace('texto: ', '').replace('descripción: ', '').replace('\"', '').replace('.', '')\n",
    "                                    sugerencias_test.append('  - funcion: comprobar_contenido_hover\\r\\n    params:\\r\\n      - xpath: \\r\\n        esperar: \\r\\n          - xpath: \\r\\n        tipo_comprobacion: y\\r\\n        contenido: \\r\\n          - xpath: //*[contains(text(), \"' + resultado_busqueda + '\")]')\n",
    "\n",
    "                        case 'aparece|contiene|Constitución|constitucion':\n",
    "                            texto_aniadible = '  - funcion: comprobar_contenido_elemento_2\\r\\n    params:\\r\\n      - xpath: \\r\\n        tipo_comprobacion: y\\r\\n        contenido: '\n",
    "                            if 'texto \"' in texto_test:\n",
    "                                textos = re.findall('texto \"\\w+\"', texto_test)\n",
    "                                j = 0\n",
    "                                while j < len(textos):\n",
    "                                    texto_aniadible = texto_aniadible + '\\r\\n          - xpath: //*[contains(text(), \"' + textos[j].replace('\"', '') + '\")]'\n",
    "                                    j = j + 1\n",
    "\n",
    "                                sugerencias_test.append(texto_aniadible)\n",
    "\n",
    "                            if 'el campo' in texto_test or 'los campos' in texto_test:\n",
    "                                textos = re.findall(r'[A-Z]{2,}[A-Z]{2,}|\\\".+\\\"', texto_test)\n",
    "                                j = 0\n",
    "                                while j < len(textos):\n",
    "                                    texto_aniadible = texto_aniadible + '\\r\\n          - xpath: //*[contains(text(), \"' + textos[j].replace('\"', '') + '\")]//following-sibling::input'\n",
    "                                    j = j + 1\n",
    "\n",
    "                                sugerencias_test.append(texto_aniadible)\n",
    "\n",
    "                        case 'habilitado|habilita':\n",
    "                            if 'botón' in texto_test:\n",
    "                                resultado_busqueda = re.search(r'botón\\s[A-Z]+.+(?:deshabilitado|\\shabilitado)', texto_test)\n",
    "                                if resultado_busqueda is not None:\n",
    "                                    resultado_busqueda = resultado_busqueda.group(0)\n",
    "                                    if ' habilitado' in resultado_busqueda:\n",
    "                                        sugerencias_test.append('  - funcion: comprobar_elemento_habilitado\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + resultado_busqueda.split()[1] + '\")]\\r\\n        habilitado: true')\n",
    "\n",
    "                                    else:\n",
    "                                        sugerencias_test.append('  - funcion: comprobar_elemento_habilitado\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + resultado_busqueda.split()[1] + '\")]\\r\\n        habilitado: false')\n",
    "\n",
    "                                else:\n",
    "                                    resultado_busqueda = re.search(r'(?:\"\\w+\"|\\(.+\\)).+(?:habilitado)', texto_test)\n",
    "                                    if resultado_busqueda is not None:\n",
    "                                        resultado_busqueda = resultado_busqueda.group(0)\n",
    "                                        texto_boton = re.search(r'(\"\\w+\"|\\(.+\\))', texto_test)\n",
    "                                        if texto_boton is not None:\n",
    "                                            texto_boton = texto_boton.group(0)\n",
    "                                            texto_boton = texto_boton.replace('\"', '').replace('()','').replace(')', '')\n",
    "\n",
    "                                            if ' habilitado' in resultado_busqueda:\n",
    "                                                sugerencias_test.append('  - funcion: comprobar_elemento_habilitado\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + texto_boton + '\")]\\r\\n        habilitado: true')\n",
    "\n",
    "                                            else:\n",
    "                                                sugerencias_test.append('  - funcion: comprobar_elemento_habilitado\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + texto_boton + '\")]\\r\\n        habilitado: false')\n",
    "\n",
    "            # Se eliminan aquellos elementos de la lista que no tengan contenido\n",
    "            while '' in sugerencias_test:\n",
    "                sugerencias_test.remove('')\n",
    "\n",
    "            if len(sugerencias_test) > 0:\n",
    "                df_modelo.at[i, 'SUGERENCIAS'] = '\\r\\n'.join(list(set(sugerencias_test)))\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Guardamos como fichero XLSX con filtrado y preparado para su edición\n",
    "    guardar_df_como_excel(df_modelo, directorio_ficheros_excel + '\\\\MODELO.xlsx', aplicar_colorizacion=False)\n",
    "\n",
    "obtener_posiciones_datos_y_sugerencias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887fa73-b451-44e7-90ed-7077a297cf4a",
   "metadata": {},
   "source": [
    "Una vez completado el proceso, se deberá completar la columna de YAML, contenedora de las clases de las diferentes instancias. Para ello, el fichero generado a partir de la exploración de la página web y las columnas de sugerencias y posibles índices serán de ayuda en esta parte del proceso.\n",
    "\n",
    "Una vez se complete el proceso, se procederá a la agrupación de casos de prueba en el test correspondiente, de manera que una vez se entrene el modelo sea posible realizar pruebas particulares o pruebas más globales. Si, por ejemplo, se tuviese un test denominado NTP y unos casos de prueba como comprobación de stratum y otro de comprobación de IP, se podría comprobar el stratum, la IP o ambos. A continuación puede verse el código utilizado para dicho propósito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5303e435-cae5-4891-b1e5-1ba4dbcdc0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmarrieta\\AppData\\Local\\Temp\\ipykernel_23120\\3880471929.py:10: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  yamls = df_modelo.loc[(df_modelo['Titulo'].str.contains(titulo)) & (df_modelo['Tipo'].str.contains('Case'))]['YAML']\n",
      "C:\\Users\\jmarrieta\\AppData\\Local\\Temp\\ipykernel_23120\\3880471929.py:16: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  indice_test = df_modelo.loc[(df_modelo['Titulo'].str.contains(titulo)) & (df_modelo['Tipo'].str.contains('Test'))]\n"
     ]
    }
   ],
   "source": [
    "def agrupar_casos_en_test():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "    df_modelo = pd.read_excel(io=directorio_ficheros_excel + 'MODELO.xlsx', sheet_name='MODELO', index_col=None)\n",
    "\n",
    "    # Obtenemos los títulos sin valores repetidos\n",
    "    titulos = df_modelo['Titulo'].unique()\n",
    "    for titulo in titulos:\n",
    "        # Obtenemos los YAML de todos los casos para ese test\n",
    "        yamls = df_modelo.loc[(df_modelo['Titulo'].str.contains(titulo)) & (df_modelo['Tipo'].str.contains('Case'))]['YAML']\n",
    "\n",
    "        # Eliminamos aquellos que únicamente tengan el esqueleto\n",
    "        yamls = [yaml for yaml in yamls if 'http' in yaml]\n",
    "\n",
    "        # Obtenemos el índice del Test con dicho título\n",
    "        indice_test = df_modelo.loc[(df_modelo['Titulo'].str.contains(titulo)) & (df_modelo['Tipo'].str.contains('Test'))]\n",
    "        if len(indice_test) > 0:\n",
    "            indice_test = indice_test.index.tolist()[0]\n",
    "\n",
    "            # Insertamos para dicho test\n",
    "            df_modelo.at[indice_test, 'YAML'] = '[\\r\\n' + ',\\r\\n\\r\\n'.join(yamls) + '\\r\\n]'\n",
    "\n",
    "    # Guardamos el modelo con los cambios\n",
    "    guardar_df_como_excel(df_modelo, directorio_ficheros_excel + 'MODELO.xlsx', aplicar_colorizacion=False)\n",
    "\n",
    "agrupar_casos_en_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c9abd-e65e-4d03-b248-07f9b371f22d",
   "metadata": {},
   "source": [
    "En la siguiente celda descargaremos los modelosHelsinki-NLP/opus-mt-es-en y Helsinki-NLP/opus-mt-en-es, que servirán para realizar Data Augmentation mediante la técnica de back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b2e241-30c9-45a5-a907-846a8e35d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|███████████████████████████████████████████████████████████████████████| 11/11 [00:00<?, ?it/s]\n",
      "Fetching 12 files: 100%|████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 11995.15it/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jmarrieta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\jmarrieta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jmarrieta\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(repo_id='Helsinki-NLP/opus-mt-es-en', cache_dir='./huggingface_mirror')\n",
    "snapshot_download(repo_id='Helsinki-NLP/opus-mt-en-es', cache_dir='./huggingface_mirror')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b12357-7f70-4895-aef6-a12330c42e64",
   "metadata": {},
   "source": [
    "Una vez se tiene un primer modelo, se han de incrementar el número de atributos para una misma clase, es decir, incrementar el número de textos que producen el mismo YAML de salida, para que así sea más posible que una vez entrenado el modelo se entiendan mejor las acciones que desea realizar el usuario. Para ello, se utilizarán técnicas de Data Augmentation con la librería nlpaug, diseñada para la realización de la tarea de forma sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec815f-6dfc-4b20-abfa-da635dfbcd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                          | 2/1039 [01:29<13:21:36, 46.38s/texto]"
     ]
    }
   ],
   "source": [
    "def aumentar_datos_tests():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "    df_modelo = pd.read_excel(io=directorio_ficheros_excel + 'MODELO.xlsx', sheet_name='MODELO', index_col=None)\n",
    "\n",
    "    # Creamos un dataframe auxiliar, de manera que el test original y aquellos generados con la aumentación queden consecutivos\n",
    "    df_modelo_aumentado = pd.DataFrame(columns=df_modelo.columns)\n",
    "\n",
    "    # Definimos el aumentador de sinónimos\n",
    "    aumentador_sinonimos = aumentador_palabras.SynonymAug(aug_src='wordnet', lang='spa')\n",
    "\n",
    "    # Definimos el aumentador mediante el método de back translation\n",
    "    aumentador_back_translation = aumentador_palabras.back_translation.BackTranslationAug(from_model_name='Helsinki-NLP/opus-mt-es-en', to_model_name='Helsinki-NLP/opus-mt-en-es', name='BackTranslationAug', device='cpu', batch_size=32, max_length=512, force_reload=False, verbose=0)\n",
    "    \n",
    "    # Obtenemos en el siguiente bucle todas las filas, e iremos aplicando la técnica fila a fila\n",
    "    barra_de_progreso = tqdm(unit='texto', total=len(df_modelo))\n",
    "    i = 0\n",
    "    while i < len(df_modelo):\n",
    "        # En primer lugar añadimos la fila existente al modelo final\n",
    "        df_modelo_aumentado.loc[len(df_modelo_aumentado)] = df_modelo.loc[i]\n",
    "\n",
    "        # Obtenemos el texto a partir del cual se generarán otros\n",
    "        texto_test = df_modelo.loc[i]['Test']\n",
    "\n",
    "        # Generamos los nuevos textos, primero mediante aumentación por sinónimos\n",
    "        nuevo_texto = aumentador_sinonimos.augment(texto_test)\n",
    "\n",
    "        # Añadimos los nuevos textos al modelo final\n",
    "        nueva_fila = df_modelo.loc[i]\n",
    "        nueva_fila['Test'] = nuevo_texto\n",
    "        \n",
    "        df_modelo_aumentado.loc[len(df_modelo_aumentado)] = nueva_fila\n",
    "        \n",
    "        # Generamos los nuevos textos, primero mediante aumentación por sinónimos\n",
    "        nuevo_texto = aumentador_back_translation.augment(texto_test, n=4, num_thread=4)\n",
    "\n",
    "        # Añadimos los nuevos textos al modelo final\n",
    "        nueva_fila = df_modelo.loc[i]\n",
    "        nueva_fila['Test'] = nuevo_texto\n",
    "        \n",
    "        df_modelo_aumentado.loc[len(df_modelo_aumentado)] = nueva_fila\n",
    "        barra_de_progreso.update(1)\n",
    "        \n",
    "        i = i + 1\n",
    "    \n",
    "    # Guardamos el modelo con los cambios\n",
    "    guardar_df_como_excel(df_modelo, directorio_ficheros_excel + 'MODELO.xlsx', aplicar_colorizacion=False)\n",
    "\n",
    "aumentar_datos_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c43a2-ba3a-4048-bd78-b3f1a27e1ca4",
   "metadata": {},
   "source": [
    "Una vez completado el proceso, se procederá a una revisión del modelo resultante. Tras ello, se procederá a guardar el modelo que se entrenará:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2667a998-9597-41cc-9c1c-a584080e37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_modelo_entrenamiento():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "    df_modelo = pd.read_excel(io=directorio_ficheros_excel + 'MODELO.xlsx', sheet_name='MODELO', index_col=None)\n",
    "    \n",
    "    # Eliminamos las columnas que utilizamos originalmente para la edición y generación del modelo\n",
    "    df_modelo.drop(columns=['Tipo', 'Titulo', 'SUGERENCIAS', 'POSIBLES_INDICES_DATOS'], inplace=True)\n",
    "\n",
    "    # Borramos aquellas filas que contienen únicamente el esqueleto, si las hubiese\n",
    "    df_modelo = df_modelo.loc[(df_modelo['YAML'] != np.NaN) &(df_modelo['YAML'].str.contains('http'))]\n",
    "\n",
    "    guardar_df_como_excel(df_modelo, directorio_ficheros_excel + 'MODELO_ENTRENAMIENTO.xlsx', aplicar_colorizacion=False)\n",
    "\n",
    "guardar_modelo_entrenamiento()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9db398-736a-4caf-b58a-644f3b8b7a36",
   "metadata": {},
   "source": [
    "Desde este momento ya se tendría el modelo para entrenamiento listo para su uso. Dicho proceso puede visualizarse en el fichero \n",
    "entrenamiento_y_uso_modelo.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
