{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9849bad-ae46-4083-a43e-b100b0649a91",
   "metadata": {},
   "source": [
    "<h1><strong>Extracción y tratamiento de datos</strong></h1>\n",
    "\n",
    "El presente notebook busca tanto la extracción como el tratamiento de los datos a partir de los cuales se construirá el dataset. En primer lugar, se realizará una extracción de los datos a partir de su ubicación originale en la herramienta IBM Doors siguiento el formato DXL (DOORS eXtension Language), un lenguaje de scripting propio del fabricante para la escritura, lectura y tratamiento de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ed4a6-3e3c-4490-9209-a5ad940ed8be",
   "metadata": {},
   "source": [
    "En primer lugar realizaremos todas las importaciones de librerías necesarias para la ejecución del código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b66ae88-bff0-443c-b465-b2fe13425c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: IPython in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from import_ipynb) (8.15.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from import_ipynb) (5.10.4)\n",
      "Requirement already satisfied: backcall in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from IPython->import_ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->import_ipynb) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->import_ipynb) (4.25.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->import_ipynb) (5.8.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.27.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (4.3.8)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (310)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->import_ipynb) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->IPython->import_ipynb) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->IPython->import_ipynb) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->IPython->import_ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from asttokens>=2.1.0->stack-data->IPython->import_ipynb) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->import_ipynb) (4.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.11)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (1.25.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (2.32.4)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.19.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easy-nlp-augmentation in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easy-nlp-augmentation) (1.25.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easy-nlp-augmentation) (2.32.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easy-nlp-augmentation) (2.1.4)\n",
      "Requirement already satisfied: nlpaug in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easy-nlp-augmentation) (1.1.11)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug->easy-nlp-augmentation) (5.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->easy-nlp-augmentation) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->easy-nlp-augmentation) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->easy-nlp-augmentation) (2023.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->easy-nlp-augmentation) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->easy-nlp-augmentation) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->easy-nlp-augmentation) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->easy-nlp-augmentation) (2023.7.22)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug->easy-nlp-augmentation) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug->easy-nlp-augmentation) (3.19.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug->easy-nlp-augmentation) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->easy-nlp-augmentation) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug->easy-nlp-augmentation) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug->easy-nlp-augmentation) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug->easy-nlp-augmentation) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (2025.7.34)\n",
      "Requirement already satisfied: click in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (1.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jmarrieta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->sacremoses) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install import_ipynb\n",
    "!pip3 install nlpaug\n",
    "!pip3 install easy-nlp-augmentation\n",
    "!pip3 install nltk\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install sacremoses\n",
    "\n",
    "import import_ipynb\n",
    "import selenium_tests\n",
    "import string\n",
    "import nlpaug.augmenter.word as aumentador_palabras\n",
    "import nlpaug.augmenter.sentence as aumentador_frases\n",
    "from easy_text_augmenter import augment_random_character\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from screeninfo import get_monitors\n",
    "import time\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "import selenium.common.exceptions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.common.alert import Alert\n",
    "from selenium.webdriver.support.relative_locator import RelativeBy\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import nltk\n",
    "import sentencepiece\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2846ff-2d5e-4a43-b289-ac2f87710d42",
   "metadata": {},
   "source": [
    "Para la extracción de los datos y guardado en fichero CSV puede consultarse el código escrito en el fichero <strong>exportacion_TP_CSV.dxl</strong>. Tras una serie de preguntas al usuario para la obtención de los datos requeridos, se hará una llamada al script dxl mencionado para la exportación de datos a CSV. Todo ello puede verse en la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4c768-8331-4984-8ea5-9dce80f5ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_cabecera_test(test):\n",
    "    return test.split('\\r\\n')[0]\n",
    "\n",
    "def generar_yaml(valor):\n",
    "    if valor == 'Case':\n",
    "        return 'URL: \\r\\n acciones: \\r\\n  - funcion:\\r\\n    params: '\n",
    "\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def obtener_titulos(tipos, tests):\n",
    "    titulos = []\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(tipos):\n",
    "        if tipos[i] == 'Test' or tipos[i] == '*':\n",
    "            titulos.append(tests[i])\n",
    "            j = i\n",
    "\n",
    "        elif tipos[i] == 'Case':\n",
    "            titulos.append(tests[j])\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    return titulos\n",
    "\n",
    "def obtener_dxl():\n",
    "    directorio_onedrive = os.getenv('OneDrive')\n",
    "    \n",
    "    # Creamos el directorio que contendrá los CSV exportados\n",
    "    directorio_exportacion = os.getcwd() + '\\\\ficheros_excel\\\\TPs\\\\'\n",
    "    os.makedirs(directorio_exportacion, exist_ok=True)\n",
    "    \n",
    "    # Se pregunta al usuario los datos para poder acceder a los módulos correspondientes dentro de IBM Doors\n",
    "    base_de_datos = input('Introduzca la base de datos a la que se ha de conectar: ')\n",
    "    usuario = input('Introduzca su nombre de usuario: ')\n",
    "    software = input('Introduzca el nombre del software cuyos casos de prueba desea extraer: ')\n",
    "    nombre_proyecto = input('Introduzca el nombre del proyecto/versión: ')\n",
    "    \n",
    "    # Dado que no es posible pasar parámetros al script DXL o no se ha sabido cómo, se guardarán los datos introducidos por el usuario\n",
    "    with open(directorio_onedrive + '\\Documents\\DOORS_y_CHANGE\\SCRIPTS DXL\\config_proyecto_sw.txt', mode='w') as f:\n",
    "        f.write(software + '\\n' + nombre_proyecto)\n",
    "    \n",
    "    # Hacemos la llamada al script exportacion_TP_CSV.dxl para la obtención de los ficheros que contienen las pruebas\n",
    "    comando_a_ejecutar = r'\"C:\\Program Files\\IBM\\Rational\\DOORS\\9.7\\bin\\doors.exe\" -d ' + base_de_datos + ' -user ' + usuario + ' -batch \"' + directorio_onedrive + '\\Documents\\DOORS_y_CHANGE\\SCRIPTS DXL\\TP_CSV_EXPORT.dxl\" -a \"\\\\' + base_de_datos.split('@')[-1] + '\\DXL\\addins;\\\\' + base_de_datos.split('@')[-1] + '\\DXL\" -J \\\\' + base_de_datos.split('@')[-1] + '\\DXL\\project\"'\n",
    "    resultado = subprocess.call(comando_a_ejecutar)\n",
    "\n",
    "    # Creamos el dataframe que contendrá todos los casos de prueba tras su lectura y procesamiento, así como las listas que contendrán las pruebas y los títulos\n",
    "    df_resultante = pd.DataFrame(columns=['Tipo', 'Test', 'Titulo'])\n",
    "    all_tests = []\n",
    "    all_titles = []\n",
    "\n",
    "    nombres_ficheros_csv = os.listdir(directorio_exportacion)\n",
    "\n",
    "    # Eliminamos de la lista todo aquel fichero que pueda existir en el directorio que no sea CSV\n",
    "    nombres_ficheros_csv = [nombre_fichero_csv for nombre_fichero_csv in nombres_ficheros_csv if nombre_fichero_csv.endswith('.csv')]\n",
    "\n",
    "    for nombre_ficheros_csv in nombres_ficheros_csv:\n",
    "        # Detectamos la codificación con que se guardaron, ya que los CSV contienen acentos y otros caracteres especiales\n",
    "        with open(directorio_exportacion + nombre_ficheros_csv, mode='rb') as f:\n",
    "            codificacion = chardet.detect(f.read())['encoding']\n",
    "\n",
    "        nombre_fichero = directorio_exportacion + nombre_ficheros_csv\n",
    "        with open(nombre_fichero, mode='r', encoding=codificacion) as f:\n",
    "            print(time.strftime('%d/%m/%Y %H:%M:%S') + '\\tProcesando ' + nombre_fichero + '...')\n",
    "            contenido_fichero = f.read()\n",
    "            contenido_fichero = contenido_fichero.split('\\n')\n",
    "\n",
    "            # Comprobamos si el caso de prueba aplica a la versión V2.01.13 o inferior. De no ser así, no exportará al CSV final\n",
    "            i = 0\n",
    "            while i < len(contenido_fichero):\n",
    "                if 'Case' in contenido_fichero[i]:\n",
    "                    j = i + 1\n",
    "                    tested_versions = []\n",
    "                    while j < len(contenido_fichero) and 'Case' not in contenido_fichero[j]:\n",
    "                        if 'Execution' in contenido_fichero[j]:\n",
    "                            tested_versions.append(contenido_fichero[j].split(';-;')[0])\n",
    "\n",
    "                        j = j + 1\n",
    "\n",
    "                    if '02.01.' not in ' '.join(tested_versions) and '02.00.' not in ' '.join(tested_versions):\n",
    "                        contenido_fichero = contenido_fichero[0:i] + contenido_fichero[j:]\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            # Eliminamos líneas que indican si la ejecución tuvo éxito o no\n",
    "            contenido_fichero = [content for content in contenido_fichero if ';-;Execution;-;' not in content]\n",
    "\n",
    "            # Eliminamos aquellas lineas referentes a tests sin casos de prueba\n",
    "            i = 0\n",
    "            while i < len(contenido_fichero):\n",
    "                if i < len(contenido_fichero) - 1 and ';Test;' in contenido_fichero[i] and ';Test;' in contenido_fichero[i + 1]:\n",
    "                    contenido_fichero.remove(contenido_fichero[i])\n",
    "\n",
    "                else:\n",
    "                    i = i + 1\n",
    "\n",
    "            # Check if the next line is part of a case, in which occurrence we add a \\n and write the value between double commas\n",
    "            # Comprobamos si la siguiente línea es parte de un caso de prueba. De ser así, añadimos una nueva línea y escribimos el texto entre comillas dobles para que en Excel sea una única celda\n",
    "            i = 2\n",
    "            while i < len(contenido_fichero):\n",
    "                if i < (len(contenido_fichero) - 1) and 'Case' not in contenido_fichero[i + 1] and 'Test' not in contenido_fichero[i + 1] and '*;' not in contenido_fichero[i + 1]:\n",
    "                    contenido_fichero[i] = contenido_fichero[i] + '\\r\\n' + contenido_fichero[i + 1]\n",
    "                    contenido_fichero.remove(contenido_fichero[i + 1])\n",
    "\n",
    "                else:\n",
    "                    i = i + 1\n",
    "\n",
    "            # Separamos las columnas. La separación original era ;-; para que las , o ; que pudiesen haber no se tomasen como separación de columnas\n",
    "            i = 0\n",
    "            while i < len(contenido_fichero):\n",
    "                contenido_fichero[i] = contenido_fichero[i].split(';-;')\n",
    "                if len(contenido_fichero[i]) > 1:\n",
    "                    contenido_fichero[i] = contenido_fichero[i][2:]\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        # Introducimos el contenido en un dataframe\n",
    "        df = pd.DataFrame(columns=contenido_fichero[0], data=contenido_fichero[1:])\n",
    "\n",
    "        # Renombramos las columnas\n",
    "        df.columns = ['Tipo', 'Test']\n",
    "\n",
    "        # Añadimos la columna Titulo, que será la cabecera del objeto, es decir, el título del caso de prueba, que en DOORS se muestra en negrita\n",
    "        df['Titulo'] = obtener_titulos(df.Tipo, df.Test)\n",
    "\n",
    "        # Añadimos el dataframe al dataframe final, contenedor de todos los casos de prueba\n",
    "        df_resultante = pd.concat([df_resultante, df], ignore_index=True)\n",
    "        l = 0\n",
    "\n",
    "    orden_nuevas_columnas = ['Tipo', 'Titulo', 'Test']\n",
    "    df_resultante = df_resultante[orden_nuevas_columnas]\n",
    "\n",
    "    # Hay casos de pruebas duplicados, donde una entrada hace referencia a una versión anterior a la que se está probando. Las buscamos y eliminamos\n",
    "    df_resultante['Cabecera caso'] = df_resultante['Test'].apply(obtener_cabecera_test)\n",
    "\n",
    "    # Comprobamos aquellos casos donde la cabecera es la misma a excepción de que una indica la versión y otra no\n",
    "    cabeceras_sin_version = [cabecera for cabecera in df_resultante['Cabecera caso'] if '(V2' not in cabecera]\n",
    "\n",
    "    for cabecera in cabeceras_sin_version:\n",
    "        # Buscamos las filas que contienen cabecera, incuyendo la versión a partir de la cual aplica\n",
    "        filas_encontradas = df_resultante.loc[(df_resultante['Test'].str.contains(cabecera + ' (V2', regex=False)) | (df_resultante['Cabecera caso'].str.contains(cabecera + '(V2', regex=False)) | (df_resultante['Cabecera caso'].str.contains(cabecera + '(v2', regex=False)) | (df_resultante['Cabecera caso'].str.contains(cabecera + '(v2', regex=False))]\n",
    "\n",
    "        if len(filas_encontradas) > 0:\n",
    "            # Obtenemos el contenido del test sin la cabecera, que se usará para comprobar que la única diferencia es la versión\n",
    "            texto_test = '\\r\\n'.join(filas_encontradas.loc[filas_encontradas.index[0]]['Test'].split('\\r\\n')[1:])\n",
    "\n",
    "            # Buscamos las filas que contienen la cabecera que estamos buscando, sin la versión, y obtenemos el índice\n",
    "            filas_originales = df_resultante.loc[df_resultante['Test'].str.contains(cabecera + '\\r\\n', regex=False)]\n",
    "            indice = filas_originales.index[0]\n",
    "\n",
    "            # Si, en efecto, se trata de filas idénticas salvo la versión en la cabecera, borramos la entrada anterior\n",
    "            if texto_test in filas_originales.loc[indice]['Test']:\n",
    "                df_resultante.drop(index=indice, inplace=True)\n",
    "\n",
    "\n",
    "    # Eliminamos la columna dado que ya no resulta necesaria\n",
    "    df_resultante.drop(columns=['Cabecera caso'], inplace=True)\n",
    "\n",
    "    # Reordenamos las columnas por el contenido de Test. De esa forma, se facilitará encontrar filas que puedan ser idénticas\n",
    "    df_resultante.sort_values(by=['Titulo', 'Test'], inplace=True)\n",
    "\n",
    "    # Ordenamos por el contenido en test\n",
    "    df_resultante.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Añadimos la columna YAML con un contenido por defecto para facilitar la edición\n",
    "    df_resultante['YAML'] = df_resultante['Tipo'].apply(generar_yaml)\n",
    "\n",
    "    csv_resultante = directorio_exportacion + '..\\\\MODELO.csv'\n",
    "    print(time.strftime('%d/%m/%Y %H:%M:%S') + '\\tGuardando archivo ' + csv_resultante)\n",
    "\n",
    "    intentar_guardar = True\n",
    "    while intentar_guardar:\n",
    "        try:\n",
    "            df_resultante.to_csv(csv_resultante, index=None, sep=';', encoding='windows-1252')\n",
    "            intentar_guardar = False\n",
    "\n",
    "        except PermissionError:\n",
    "            print('¡Error intentando guardar ' + csv_resultante + '! Cierre el fichero para poder guardarlo')\n",
    "            time.sleep(15)\n",
    "\n",
    "obtener_dxl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba1666-976a-4433-b5ac-e6dc0339c899",
   "metadata": {},
   "source": [
    "En este momento se habrán exportado todos los casos de prueba y almacenado como CSV. Antes de continuar, se procederá a comprobar que los datos se extrajeron correctamente.\n",
    "\n",
    "Tras dicha revisión, se procede a realizar un primer estudio de los datos, donde extraeremos la cantidad de palabras distintas y apariciones, cuántos casos de prueba tienen una longitud mayor de 512 caracteres y la longitud mayor encontrada entre los diversos casos de prueba. Todo ello será de utilidad para la elección del modelo pre-entrenado que mejores resultados pueda ofrecer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8022aee-6558-4199-ab3e-938553e31534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de entradas de texto, incluyendo títulos: 1039\n",
      "\n",
      "Palabras diferentes: 2882\n",
      "\n",
      "Palabras distintas, con todas las letras en minúsculas: 2356\n",
      "\n",
      "Entradas con longitud superior a 512 palabras: 0 \n",
      "Mayor longitud encontrada para texto de entrada: 441 . Índice:  812\n"
     ]
    }
   ],
   "source": [
    "def analizar_datos(nombre_fichero):\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\\n",
    "    # Se abre el fichero para su análisis\n",
    "    if nombre_fichero.upper().endswith('CSV'):\n",
    "        df = pd.read_csv(directorio_ficheros_excel + nombre_fichero, sep=';', encoding='windows-1252')\n",
    "\n",
    "    elif\n",
    "        df = pd.read_excel(io=directorio_ficheros_excel + nombre_fichero, sheet_name=nombre_fichero.split('.')[0], index_col=None)\n",
    "    \n",
    "    else:\n",
    "        print('No se ha podido abrir el fichero indicado para analizar datos.')\n",
    "    \n",
    "    # Definimos diccionarios, cuyas claves serán las diferentes palabras encontradas y su valor la cantidad de veces que se encontró\n",
    "    conteo_palabras = {}\n",
    "    conteo_palabras_minusculas = {}\n",
    "    \n",
    "    longitud_entrada_mas_larga = 0\n",
    "    \n",
    "    # Se extraen los valores de la columna de Test, pues contiene los textos a analizar\n",
    "    valores = df['Test'].values.tolist()\n",
    "    \n",
    "    # Se introduce el conteo de palabras en el diccionario\n",
    "    i = 0\n",
    "    while i < len(valores):\n",
    "        palabras = re.findall(r'(\\b[a-zA-Z0-9]+\\b)', valores[i])\n",
    "        for palabra in palabras:\n",
    "            try:\n",
    "                conteo_palabras[palabra] = conteo_palabras[palabra] + 1\n",
    "                conteo_palabras_minusculas[palabra.lower()] = conteo_palabras_minusculas[palabra.lower()] + 1\n",
    "    \n",
    "            except KeyError:\n",
    "                conteo_palabras[palabra] = 1\n",
    "                conteo_palabras_minusculas[palabra.lower()] = 1\n",
    "    \n",
    "        i = i + 1\n",
    "    \n",
    "    #valores = df['Test'].values.tolist()\n",
    "    i = 0\n",
    "    indice_entrada_mas_larga = 0\n",
    "    num_entradas_mayores_512_palabras = 0\n",
    "    while i < len(valores):\n",
    "        entrada_texto = valores[i].split()\n",
    "    \n",
    "        # Eliminamos signos de puntuación dado que no cuentan como tokens\n",
    "        while '.' in entrada_texto:\n",
    "            entrada_texto.remove(entrada_texto[entrada_texto.index('.')])\n",
    "    \n",
    "        while ',' in entrada_texto:\n",
    "            entrada_texto.remove(entrada_texto[entrada_texto.index(',')])\n",
    "    \n",
    "        while ';' in entrada_texto:\n",
    "            entrada_texto.remove(entrada_texto[entrada_texto.index(';')])\n",
    "    \n",
    "        while '_' in entrada_texto:\n",
    "            entrada_texto.remove(entrada_texto[entrada_texto.index('_')])\n",
    "    \n",
    "        while '-' in entrada_texto:\n",
    "            entrada_texto.remove(entrada_texto[entrada_texto.index('-')])\n",
    "    \n",
    "        while '\"' in entrada_texto:\n",
    "            entrada_texto.remove(entrada_texto[entrada_texto.index('\"')])\n",
    "    \n",
    "        while '\\'' in entrada_texto:\n",
    "            entrada_texto.remove(entrada_texto[entrada_texto.index('\\'')])\n",
    "    \n",
    "        if len(entrada_texto) > longitud_entrada_mas_larga:\n",
    "            longitud_entrada_mas_larga = len(valores[i].split())\n",
    "            indice_entrada_mas_larga = i\n",
    "    \n",
    "        if len(entrada_texto) > 512:\n",
    "            num_entradas_mayores_512_palabras = num_entradas_mayores_512_palabras + 1\n",
    "    \n",
    "        i = i + 1\n",
    "    \n",
    "    print('Cantidad total de entradas de texto, incluyendo títulos:', len(valores))\n",
    "    print('\\nPalabras diferentes: ' + str(len(conteo_palabras.keys())))\n",
    "    print('\\nPalabras distintas, con todas las letras en minúsculas: ' + str(len(conteo_palabras_minusculas.keys())))\n",
    "    print('\\nEntradas con longitud superior a 512 palabras:', num_entradas_mayores_512_palabras,\n",
    "          '\\nMayor longitud (en palabras o tokens) encontrada para texto de entrada:', longitud_entrada_mas_larga, '. Índice: ',\n",
    "          indice_entrada_mas_larga)\n",
    "    \n",
    "    with open(os.getcwd() + '\\\\ficheros_excel\\\\conteo_datos.csv', mode='w', encoding='windows-1252') as f:\n",
    "        f.write('Palabra;Veces escrita\\n')\n",
    "        for key_palabra in conteo_palabras.keys():\n",
    "            f.write(key_palabra + ';' + str(conteo_palabras[key_palabra]) + '\\n')\n",
    "    \n",
    "    with open(os.getcwd() + '\\\\ficheros_excel\\\\conteo_datos_minúsculas.csv', mode='w', encoding='windows-1252') as f:\n",
    "        f.write('Palabra;Veces escrita\\n')\n",
    "        for key_palabra in conteo_palabras_minusculas.keys():\n",
    "            f.write(key_palabra + ';' + str(conteo_palabras_minusculas[key_palabra]) + '\\n')\n",
    "\n",
    "analizar_datos('MODELO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b3a85-fae0-4fc9-a2cb-777c36a6a7df",
   "metadata": {},
   "source": [
    "Tras el primer análisis de los datos, procederemos a analizar la página web sobre la que se realizarán pruebas. Para ello, codificaremos funciones que ayudarán en la extracción de xpaths y capturas de imagen asociadas, tanto de forma voraz donde se intentará de forma automática realizar clics y clics derecho de ratón o ponerlo sobre elementos para así obtener el mayor contenido posible como manualmente, donde se indicará el xpath a partir del cual obtener la información y que podrá servir para completar la obtención de todos aquellos elementos que puedan existir.\n",
    "\n",
    "Antes de dichas funciones, se codifica aquella que se encargará de guardar los resultados obtenidos en la obtención de los diversos xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01eafdb9-8208-4ef2-b93c-b39879e47389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_df_como_excel(df, nombre_fichero, aplicar_filtro=True, aplicar_colorizacion=True, contiene_imagenes=False):\n",
    "    intentar_guardar = True\n",
    "\n",
    "    # Creamos una lista vacía que contendrá los DataFrames, pudiendo ser uno o varios en función del número de filas que tenga\n",
    "    # El motivo de esta acción es, además de evitar que supere el tamaño máximo permitido, poder abrir el fichero en excel\n",
    "    dfs = []\n",
    "    if len(df) > 10000:\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "            if i + 10000 < len(df):\n",
    "                dfs.append(df[i:i + 10000])\n",
    "\n",
    "            else:\n",
    "                dfs.append(df[i:])\n",
    "\n",
    "            i = i + 10000\n",
    "\n",
    "    else:\n",
    "        dfs.append(df)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(dfs):\n",
    "        while intentar_guardar:\n",
    "            try:\n",
    "                if len(dfs) > 1:\n",
    "                    excel_writer = pd.ExcelWriter(nombre_fichero.replace('.xlsx', '_' + str(i) + '.xlsx'))\n",
    "\n",
    "                else:\n",
    "                    excel_writer = pd.ExcelWriter(nombre_fichero)\n",
    "\n",
    "                nombre_hoja = nombre_fichero.replace('.xlsx', '')\n",
    "                if '\\\\' in nombre_hoja:\n",
    "                    nombre_hoja = nombre_hoja.split('\\\\')[-1]\n",
    "\n",
    "                dfs[i].to_excel(excel_writer=excel_writer, sheet_name=nombre_hoja, index=None)\n",
    "                workbook = excel_writer.book\n",
    "                worksheet = excel_writer.sheets[nombre_hoja]\n",
    "\n",
    "                # Calculamos el ancho de la columna, que ayudará a poder visualizar el contenido sin tener que modificar en excel\n",
    "                columnas_hoja = dfs[i].columns.tolist()\n",
    "                for nombre_columna in columnas_hoja:\n",
    "                    ancho_columna = len(nombre_columna)\n",
    "                    valores_columna = dfs[i][nombre_columna]\n",
    "\n",
    "                    for texto_celda in valores_columna:\n",
    "                        if type(texto_celda) is str and '\\r\\n' in texto_celda:\n",
    "                            texto_celda = texto_celda.split('\\r\\n')\n",
    "                            long_max_valores_columna = len(max(texto_celda, key=len))\n",
    "\n",
    "                        elif type(texto_celda) is str and '\\n' in texto_celda:\n",
    "                            texto_celda = texto_celda.split('\\n')\n",
    "                            long_max_valores_columna = len(max(texto_celda, key=len))\n",
    "\n",
    "                        elif type(texto_celda) is str:\n",
    "                            long_max_valores_columna = len(texto_celda)\n",
    "\n",
    "                        if long_max_valores_columna > ancho_columna:\n",
    "                            ancho_columna = long_max_valores_columna\n",
    "\n",
    "                        # En caso de que se llegue a una columna a una longitud superior a 150, establecemos la anchura máxima a 150 y pasamos a procesar la siguiente columna\n",
    "                        if ancho_columna > 150:\n",
    "                            ancho_columna = 150\n",
    "                            break\n",
    "\n",
    "                    longitud_columna = ancho_columna + 6\n",
    "                    posicion_columna = columnas_hoja.index(nombre_columna)\n",
    "                    worksheet.set_column(posicion_columna, posicion_columna, longitud_columna)\n",
    "\n",
    "                # Colorear celdas de filas impares si se indica por parámetro. Puede ser útil para mejorar visualización del fichero excel\n",
    "                if aplicar_colorizacion:\n",
    "                    formato_celda = workbook.add_format()\n",
    "                    formato_celda.set_bg_color('c5d9f1')\n",
    "\n",
    "                    j = 1\n",
    "                    rows_indexes = df.index.tolist()\n",
    "                    while j < len(df):\n",
    "                        if j % 2 != 0:\n",
    "                            k = 0\n",
    "                            while k < len(columnas_hoja):\n",
    "                                value_to_modify = df.loc[rows_indexes[j]][columnas_hoja[k]]\n",
    "                                worksheet.write(j + 1, k, value_to_modify, formato_celda)\n",
    "                                k = k + 1\n",
    "\n",
    "                        j = j + 1\n",
    "\n",
    "                # Aplicar filtros si se indica, equivalente a ir a Datos -> Filtro en Excel\n",
    "                if aplicar_filtro:\n",
    "                    worksheet.autofilter(0, 0, len(df), len(columnas_hoja) - 1)\n",
    "\n",
    "                # Sustituimos el nombre del fichero en la columna PNG por el contenido de la imagen en la propia celda\n",
    "                if contiene_imagenes:\n",
    "                    fila = 1\n",
    "                    while fila <= len(df):\n",
    "                        fichero_imagen = df.loc[fila-1]['PNG']\n",
    "                        if fichero_imagen is not None and fichero_imagen != '':\n",
    "                            try:\n",
    "                                worksheet.embed_image(fila, df.columns.tolist().index('PNG'), fichero_imagen)\n",
    "\n",
    "                            except (FileNotFoundError, Exception) as e:\n",
    "                                print(e)\n",
    "                                pass\n",
    "\n",
    "                        fila = fila + 1\n",
    "\n",
    "                # Terminamos de guardar el fichero. Si fuese bien, se establece intentar guardar a False para salir del bucle\n",
    "                excel_writer.close()\n",
    "                intentar_guardar = False\n",
    "\n",
    "            except PermissionError:\n",
    "                print('¡Error, parece que el fichero ' + nombre_fichero + ' se encuentra abierto! Cierre el fichero para poder guardarlo')\n",
    "                time.sleep(10)\n",
    "\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47117c0e-5ebe-41ee-8083-b26cfbb20032",
   "metadata": {},
   "source": [
    "Por otra parte, se muestra a continuación el código utilizado para el acceso al software, inicio y cierre de sesión para poder posteriormente extraer información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec14deb-71bb-4a0f-83c7-e7d49183de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from screeninfo import get_monitors\n",
    "\n",
    "# Dado que el Software implementado está diseñado para pantallas de 4K, obtenemos con ayuda de esta función las pantallas disponibles y su resolución\n",
    "def obtener_info_pantallas():\n",
    "    monitores = get_monitors()\n",
    "\n",
    "    propiedades_monitor = None\n",
    "    ancho_monitor = 0\n",
    "    alto_monitor = 0\n",
    "    i = 0\n",
    "    while i < len(monitores):\n",
    "        if monitores[i].width > ancho_monitor and monitores[i].height > alto_monitor:\n",
    "            propiedades_monitor = monitores[i]\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    return propiedades_monitor\n",
    "\n",
    "# Obtenemos las propiedades del monitor y creamos un objeto de Tests, a cuyo constructor pasamos como parámetro las propiedades del monitor con mayor resolución\n",
    "propiedades_monitor = obtener_info_pantallas()\n",
    "tests = selenium_tests.Tests(propiedades_monitor)\n",
    "\n",
    "# Configuramos el driver\n",
    "tests.configurar_driver()\n",
    "\n",
    "# Iniciamos la sesión\n",
    "tests.iniciar_sesion(params={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aed150-0f64-4954-838b-386f987c8b92",
   "metadata": {},
   "source": [
    "Los siguientes métodos serán los utilizados para la extracción de contenido de la página web sobre la que se realizarán las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba302ada-34d9-4b59-94f3-df7ebb274d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_df_xpaths():\n",
    "    df_xpaths = pd.DataFrame(columns=['XPATH', 'PNG', 'ACCESO DESDE', 'PALABRAS CLAVE', 'CONTENIDO HTML', 'XPATHS_CONTENIDO_HTML', 'REVISADO'])\n",
    "    datos_elementos_no_repetibles = None\n",
    "\n",
    "    # Comprobamos si ya existen datos almacenados para no empezar desde cero innecesariamente\n",
    "    try:\n",
    "        with open('df_xpaths.pickle', mode='rb') as fichero_pickle:\n",
    "            df_xpaths = pickle.load(fichero_pickle)\n",
    "\n",
    "            # Creamos unas listas de elementos cuyos valores y etiquetas o xpaths son cambiantes y bastaría con tener uno a modo de ejemplo\n",
    "            datos_elementos_no_repetibles = ['class=\"app\"', 'hist-cell TRANSPARENT', 'Total flights', 'hist-cell ifr-arr', 'A2SA', 'A1SR4', 'GCCC', 'PREOPS', 'nmOTMVPeriodPeak', 'timeline', 'rowFL']\n",
    "\n",
    "            for dato_no_repetible in datos_elementos_no_repetibles:\n",
    "                existe = df_xpaths['XPATH'].str.contains(dato_no_repetible)\n",
    "                if existe.any() is False:\n",
    "                    datos_elementos_no_repetibles.remove(dato_no_repetible)\n",
    "\n",
    "    # Si se produce un error, se debe a que no existe el fichero. Lo capturamos y no hacemos nada\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    return df_xpaths\n",
    "\n",
    "def obtener_xpath_de_html(contenido_html):\n",
    "    contenido_html = contenido_html.split('<')\n",
    "    xpath_a_devolver = '//' + re.sub(r'data.+\"\"\\s', '', contenido_html[1][:contenido_html[1].index('>')])\n",
    "\n",
    "    xpaths_especiales = ['svg', 'path']\n",
    "    if xpath_a_devolver.split()[0].replace('//', '') in xpaths_especiales:\n",
    "        xpath_a_devolver = xpath_a_devolver.replace(xpath_a_devolver.split()[0], '//*[name()=\"' + xpath_a_devolver.split()[0].replace('//', '') + '\"]')\n",
    "\n",
    "    xpath_a_devolver = re.sub(r'\"\\s', '\"][@', xpath_a_devolver)\n",
    "    xpath_a_devolver = xpath_a_devolver.replace(' ', '[@', 1)\n",
    "    if '[' in xpath_a_devolver:\n",
    "        xpath_a_devolver = xpath_a_devolver + ']'\n",
    "\n",
    "    try:\n",
    "        texto_elemento = contenido_html[1]\n",
    "        texto_elemento = texto_elemento[texto_elemento.index('>') + 1:]\n",
    "        texto_elemento = re.sub(r'^\\s{1,}|\\s{1,}$', '', texto_elemento)\n",
    "        if len(texto_elemento) > 0 and re.search(r'(text\\(\\)\\,\\s\"\\d+:\\d+)', xpath_a_devolver) is None and re.search(r'(text\\(\\),\\s\"\\d+\\/\\d+\\/\\d+)', xpath_a_devolver) is None:\n",
    "            xpath_a_devolver = xpath_a_devolver + '[contains(text(), \"' + texto_elemento + '\")]'\n",
    "\n",
    "    except (IndexError, ValueError):\n",
    "        # Se da el error si es un único tag sin otro a continuación o sin texto: No necesitamos hacer nada, por lo que puede ignorarse\n",
    "        pass\n",
    "\n",
    "    return xpath_a_devolver\n",
    "\n",
    "\n",
    "def obtener_xpaths_de_html(contenido_html):\n",
    "    posibles_xpaths = []\n",
    "\n",
    "    contenido_html_en_lista = contenido_html.split('><')\n",
    "    contenido_html_en_lista = [('<' + contenido) if contenido.startswith('<') is False else contenido for contenido in contenido_html_en_lista]\n",
    "    contenido_html_en_lista = [(contenido + '>') if contenido.endswith('>') is False else contenido for contenido in contenido_html_en_lista]\n",
    "\n",
    "    # Generamos en primer lugar un xpath a partir de cada una de las etiquetas\n",
    "    i = 0\n",
    "    while i < len(contenido_html_en_lista):\n",
    "        if contenido_html_en_lista[i].startswith('<!') is False:\n",
    "            posibles_xpaths.append(obtener_xpath_de_html(contenido_html_en_lista[i]))\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    l = 0\n",
    "    return posibles_xpaths\n",
    "\n",
    "\n",
    "def obtener_xpath_de_elemento(elemento):\n",
    "    excepciones_nombre_tag = ['svg', 'path']\n",
    "    if elemento.tag_name not in excepciones_nombre_tag:\n",
    "        xpath_a_devolver = '//' + elemento.tag_name\n",
    "\n",
    "    else:\n",
    "        xpath_a_devolver = '//*[name()=\"' + elemento.tag_name + '\"]'\n",
    "\n",
    "    atributos = tests.driver.execute_script('var items = {}; for (index = 0; index < arguments[0].attributes.length; ++index) { items[arguments[0].attributes[index].name] = arguments[0].attributes[index].value }; return items;', elemento)\n",
    "    if atributos is not None and len(atributos.keys()) > 0:\n",
    "        # Comprobamos en primer lugar si el elemento tuviese un id, en cuyo caso se toma solo ese atributo dado que es un valor único\n",
    "        if 'id' in atributos:\n",
    "            xpath_a_devolver = xpath_a_devolver + '[@id=\"' + elemento.get_attribute('id') + '\"]'\n",
    "\n",
    "        else:\n",
    "            for atributo in atributos:\n",
    "                if elemento.get_attribute(atributo) is not None and elemento.get_attribute(atributo) != '':\n",
    "                    xpath_a_devolver = xpath_a_devolver + '[@' + atributo + '=\"' + elemento.get_attribute(atributo) + '\"]'\n",
    "\n",
    "    try:\n",
    "        outer_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"', '', elemento.get_attribute('outerHTML'))\n",
    "        texto_elemento = outer_html.split('<')[1]\n",
    "        texto_elemento = texto_elemento[texto_elemento.index('>') + 1:]\n",
    "        texto_elemento = re.sub(r'^\\s{1,}|\\s{1,}$', '', texto_elemento)\n",
    "        if 'Last Update' in elemento.text and '\\n' not in elemento.text:\n",
    "            l = 0\n",
    "\n",
    "        if len(texto_elemento) > 0 and re.search(r'(text\\(\\)\\,\\s\"\\d+:\\d+)', xpath_a_devolver) is None and re.search(r'(text\\(\\),\\s\"\\d+\\/\\d+\\/\\d+)', xpath_a_devolver) is None:\n",
    "            xpath_a_devolver = xpath_a_devolver + '[contains(text(), \"' + texto_elemento + '\")]'\n",
    "\n",
    "    except (IndexError, ValueError):\n",
    "        # Se da el error si es un único tag sin otro a continuación: No necesitamos hacer nada, por lo que puede ignorarse\n",
    "        pass\n",
    "\n",
    "    return xpath_a_devolver\n",
    "\n",
    "\n",
    "def obtener_xpaths_voraz():\n",
    "    datos_elementos_no_repetibles = None\n",
    "\n",
    "    # Obtenemos el dataframe con los xpaths, que estará vacío en caso de ser la primera ejecución\n",
    "    df_xpaths = obtener_df_xpaths()\n",
    "\n",
    "    # Creamos el directorio que contendrá las capturas. De existir el directorio no se hace nada más\n",
    "    os.makedirs(os.getcwd() + '\\\\capturas', exist_ok=True)\n",
    "\n",
    "    # Se espera a detectar que existe el contenedor cuya clase contiene el texto tab-ws\n",
    "    params = {'esperar': {'xpath': '//div[contains(@class,\"tab-ws\")]'}, 'tipo_espera': 'presencia'}\n",
    "    tests.esperar_elemento(params)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Preguntamos al usuario la URL de acceso\n",
    "    url_acceso = input('Indique la URL a la que se debe acceder:' )\n",
    "    tests.driver.get(url_acceso)\n",
    "\n",
    "    # En caso de estar actualizándose los datos, esperamos a que finalice\n",
    "    params = {'esperar': {'xpath': '//div[@class=\"v-card__text\"][contains(text(), \"Please stand by\")]'}, 'tipo_espera': 'invisibilidad'}\n",
    "    tests.esperar_elemento(params)\n",
    "\n",
    "    # Guardamos el contenido HTML, que nos servirá posteriormente para detectar otros posibles xpaths\n",
    "    # Sustituimos valores de fecha y hora, ya que pueda hacer que parezca que dos etiquetas son distintas cuando la diferencia es la hora actial\n",
    "    contenido_html = re.sub(r'\\d+:\\d+|\\d+\\/\\d+\\/\\d+', '', tests.driver.page_source)\n",
    "    contenido_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', contenido_html)\n",
    "\n",
    "    try:\n",
    "        if len(df_xpaths) == 0:\n",
    "            # Buscamos todos los elementos que haya en la página\n",
    "            elementos_encontrados = tests.driver.find_elements(By.XPATH, '//body//*')\n",
    "\n",
    "            tags_a_eliminar = ['html', 'link', 'title', 'style', 'script', 'head', 'meta']\n",
    "            elementos_encontrados = [elemento for elemento in elementos_encontrados if elemento is not None and elemento.tag_name is not None and elemento.tag_name not in tags_a_eliminar]\n",
    "\n",
    "            for elemento in tqdm(elementos_encontrados, unit='xpath', total=len(elementos_encontrados)):\n",
    "                try:\n",
    "                    xpath_a_aniadir = obtener_xpath_de_elemento(elemento)\n",
    "\n",
    "                    if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():\n",
    "                        try:\n",
    "                            contenido_html_sin_separar = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elemento.get_attribute('outerHTML'))\n",
    "                            xpaths_contenido_html = obtener_xpaths_de_html(contenido_html_sin_separar)\n",
    "\n",
    "                            # Obtenemos las palabras clave, que serán de utilidad para poder buscar los xpaths necesarios\n",
    "                            palabras_clave = re.findall(r'\\b\\w+\\b', xpath_a_aniadir)\n",
    "                            palabras_clave.append(re.sub(r'\\d+\\/\\d+\\/\\d+|\\d{2}:\\d{2}', '', elemento.text).replace('\\n\\n', ',').replace('\\n', ','))\n",
    "\n",
    "                            j = 10\n",
    "                            while j < len(palabras_clave):\n",
    "                                palabras_clave.insert(j, '\\n')\n",
    "\n",
    "                                j = j + 10\n",
    "\n",
    "                            # Eliminamos los valores repetidos, ya que en este caso no es necesario tener valores duplicados\n",
    "                            palabras_clave = list(set(palabras_clave))\n",
    "                            palabras_clave = ','.join(palabras_clave)\n",
    "\n",
    "                            # Guardamos la imagen en un fichero, cuyo nombre será el del índice si se hizo captura\n",
    "                            if type(elemento.screenshot_as_png) is bytes:\n",
    "                                nombre_fichero_imagen = os.getcwd() + '\\\\capturas\\\\captura' + str(len(df_xpaths)) + '.png'\n",
    "                                elemento.screenshot(nombre_fichero_imagen)\n",
    "                                df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, nombre_fichero_imagen, '', palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                            else:\n",
    "                                df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', '',  palabras_clave, contenido_html_sin_separar,  '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                        except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                            df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', '',  palabras_clave, contenido_html_sin_separar, '', '']\n",
    "\n",
    "                except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                    pass\n",
    "\n",
    "        # Convertimos contenido_html a lista, pues posteriormente se utilizará para comprobar si acciones como un clic hace que se muestre nuevo contenido\n",
    "        contenido_html = contenido_html.split('><')\n",
    "        contenido_html = [('<' + contenido) if contenido.startswith('<') is False else contenido for contenido in contenido_html]\n",
    "        contenido_html = [(contenido + '>') if contenido.endswith('>') is False else contenido for contenido in contenido_html]\n",
    "\n",
    "        acciones = ['clic_derecho', 'hover', 'clic']\n",
    "\n",
    "        # Ya obtenidos los xpath de la web, ejecutamos las acciones de clic, clic derecho y hover y buscamos posibles xpaths a añadir\n",
    "        barra_progreso = tqdm(total=len(df_xpaths), unit='xpath(s)')\n",
    "        i = 0\n",
    "        while i < len(df_xpaths):\n",
    "            if df_xpaths.loc[i]['REVISADO'] != 'SI':\n",
    "                for accion in acciones:\n",
    "                    if accion == acciones[-1]:\n",
    "                        # Lo marcamos como revisado para, en caso de retomar el proceso, no volver a hacer lo ya realizado\n",
    "                        df_xpaths.loc[i]['REVISADO'] = 'SI'\n",
    "\n",
    "                    try:\n",
    "                        # En caso de estar actualizándose los datos, esperamos a que finalice\n",
    "                        params = {'esperar': {'xpath': '//div[@class=\"v-card__text\"][contains(text(), \"Please stand by\")]'}, 'tipo_espera': 'invisibilidad'}\n",
    "                        tests.esperar_elemento(params)\n",
    "\n",
    "                        match accion:\n",
    "                            case 'clic':\n",
    "                                # Hacemos clic sobre el elemento\n",
    "                                tests.hacer_clic({'xpath': df_xpaths.loc[i]['XPATH']})\n",
    "\n",
    "                            case 'clic_derecho':\n",
    "                                # Hacemos clic derecho sobre el elemento\n",
    "                                tests.hacer_clic_derecho({'xpath': df_xpaths.loc[i]['XPATH']})\n",
    "\n",
    "                            case 'hover':\n",
    "                                 # Intentamos hacer hover sobre el elemento\n",
    "                                 tests.hacer_hover({'xpath': df_xpaths.loc[i]['XPATH']})\n",
    "\n",
    "                        # Obtenemos el nuevo contenido HTML eliminando datos de fecha y/o hora y comparamos con el almacenado para ver si es diferente\n",
    "                        # En primer lugar, eliminamos etiquetas de comentarios y datos de fecha y hora que puedan hacer que hay diferencias que para análisis no deberían existir en realidad\n",
    "                        nuevo_contenido_html = re.sub(r'\\d+:\\d+|\\d+\\/\\d+\\/\\d+', '', tests.driver.page_source).replace('<!---->', '')\n",
    "                        nuevo_contenido_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', nuevo_contenido_html)\n",
    "                        nuevo_contenido_html = nuevo_contenido_html.split('><')\n",
    "                        nuevo_contenido_html = [('<' + contenido) if contenido.startswith('<') is False else contenido for contenido in nuevo_contenido_html]\n",
    "                        nuevo_contenido_html = [(contenido + '>') if contenido.endswith('>') is False else contenido for contenido in nuevo_contenido_html]\n",
    "\n",
    "                        # Eliminamos aquello que no sea diferente o nuevo\n",
    "                        nuevo_contenido_html = [contenido for contenido in nuevo_contenido_html if contenido not in contenido_html]\n",
    "\n",
    "                        # Eliminamos aquellos elementos que puedan considerarse repetidos\n",
    "                        j = 0\n",
    "                        while j < len(datos_elementos_no_repetibles):\n",
    "                            k = 0\n",
    "                            while k < len(nuevo_contenido_html):\n",
    "                                if nuevo_contenido_html[k] in datos_elementos_no_repetibles[j]:\n",
    "                                    datos_elementos_no_repetibles.remove(datos_elementos_no_repetibles[j])\n",
    "\n",
    "                                else:\n",
    "                                    k = k + 1\n",
    "\n",
    "                            j = j + 1\n",
    "\n",
    "                        for contenido in nuevo_contenido_html:\n",
    "                            xpath_a_aniadir = obtener_xpath_de_html(contenido)\n",
    "\n",
    "                            if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():\n",
    "                                try:\n",
    "                                    # Buscamos todos los elementos que haya en la página\n",
    "                                    elementos_encontrados = tests.driver.find_elements(By.XPATH, xpath_a_aniadir + '//*')\n",
    "                                    elementos_encontrados = [elemento for elemento in elementos_encontrados if elemento.tag_name not in tags_a_eliminar]\n",
    "\n",
    "                                    for elemento in elementos_encontrados:\n",
    "                                        try:\n",
    "                                            xpath_a_aniadir = obtener_xpath_de_elemento(elemento)\n",
    "\n",
    "                                            if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():# and '[' in xpath_a_aniadir:\n",
    "                                                try:\n",
    "                                                    contenido_html_sin_separar = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elemento.get_attribute('outerHTML'))\n",
    "                                                    xpaths_contenido_html = obtener_xpaths_de_html(contenido_html_sin_separar)\n",
    "\n",
    "                                                    # Obtenemos las palabras clave, que serán de utilidad para poder buscar los xpaths necesarios\n",
    "                                                    palabras_clave = re.findall(r'\\b\\w+\\b', xpath_a_aniadir)\n",
    "                                                    palabras_clave.append(re.sub(r'\\d+\\/\\d+\\/\\d+|\\d{2}:\\d{2}', '', elemento.text).replace('\\n\\n', ',').replace('\\n', ','))\n",
    "\n",
    "                                                    j = 10\n",
    "                                                    while j < len(palabras_clave):\n",
    "                                                        palabras_clave.insert(j, '\\n')\n",
    "\n",
    "                                                        j = j + 10\n",
    "\n",
    "                                                    # Eliminamos los valores repetidos, ya que en este caso no es necesario tener valores duplicados\n",
    "                                                    palabras_clave = list(set(palabras_clave))\n",
    "                                                    palabras_clave = ','.join(palabras_clave)\n",
    "\n",
    "                                                    # Si el botón de Cancelar, Cerrar o No está presente y estamos en la acción de clic, haremos clic sobre él para poder continuar\n",
    "                                                    if accion == 'clic' and ('CANCEL' in xpath_a_aniadir.upper()) or ('CLOSE' in xpath_a_aniadir.upper()) or ('NO' in xpath_a_aniadir.upper()):\n",
    "                                                        tests.hacer_clic({'xpath': xpath_a_aniadir})\n",
    "\n",
    "                                                    # Guardamos la imagen en un fichero, cuyo nombre será el del índice si se hizo captura\n",
    "                                                    if type(elemento.screenshot_as_png) is bytes:\n",
    "                                                        nombre_fichero_imagen = os.getcwd() + '\\\\capturas\\\\captura' + str(len(df_xpaths)) + '.png'\n",
    "                                                        df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, nombre_fichero_imagen, df_xpaths.loc[i]['XPATH'], palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                                                        # Guardamos la imagen en un fichero, cuyo nombre será el del índice\n",
    "                                                        elemento.screenshot(nombre_fichero_imagen)\n",
    "\n",
    "                                                except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                                                    df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', df_xpaths.loc[i]['XPATH'], palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                                        except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException, selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                                            pass\n",
    "\n",
    "                                except (selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.InvalidSelectorException, selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                                    pass\n",
    "\n",
    "                    except (AttributeError, selenium.common.exceptions.ElementNotInteractableException,selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                        pass\n",
    "\n",
    "                    # En caso de que suceda algún error no contemplado, lo capturamos para poder guardar todo lo que se haya encontrado\n",
    "                    except Exception:\n",
    "                        # Guardamos fichero pickle para, en caso de volver a ejecutar, poder hacerlo desde donde nos hubiésemos quedado\n",
    "                        with open('df_xpaths.pickle', mode='wb') as fichero_pickle:\n",
    "                            pickle.dump(df_xpaths, fichero_pickle)\n",
    "\n",
    "                        # Acciones como CTRL+C provocan este error. Lo capturamos para poder guardar lo hecho y por si se quiere revisar durante el proceso que va por buen camino\n",
    "                        print('Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido')\n",
    "                        guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)\n",
    "\n",
    "            i = i + 1\n",
    "            barra_progreso.total = len(df_xpaths)\n",
    "            barra_progreso.update(1)\n",
    "\n",
    "        barra_progreso.close()\n",
    "\n",
    "        # Guardamos el fichero Excel\n",
    "        guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Guardamos fichero pickle para, en caso de volver a ejecutar, poder hacerlo desde donde nos hubiésemos quedado\n",
    "        with open('df_xpaths.pickle', mode='wb') as fichero_pickle:\n",
    "            pickle.dump(df_xpaths, fichero_pickle)\n",
    "\n",
    "        # Acciones como CTRL+C provocan este error. Lo capturamos para poder guardar lo hecho y por si se quiere revisar durante el proceso que va por buen camino\n",
    "        print('Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido')\n",
    "        guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)\n",
    "\n",
    "\n",
    "def obtener_xpaths_asistido():\n",
    "    datos_elementos_no_repetibles = None\n",
    "\n",
    "    # Obtenemos el dataframe con los xpaths, que estará vacío en caso de ser la primera ejecución\n",
    "    df_xpaths = obtener_df_xpaths()\n",
    "\n",
    "    # Creamos el directorio que contendrá las capturas. De ya existir, no se hace nada\n",
    "    os.makedirs(os.getcwd() + '\\\\capturas', exist_ok=True)\n",
    "\n",
    "    params = {'esperar': {'xpath': '//div[contains(@class,\"tab-ws\")]'}, 'tipo_espera': 'presencia'}\n",
    "    tests.esperar_elemento(params)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Preguntamos al usuario la URL de acceso\n",
    "    url_acceso = input('Indique la URL a la que se debe acceder:' )\n",
    "    tests.driver.get(url_acceso)\n",
    "\n",
    "    # En caso de estar actualizándose los datos, esperamos a que finalice\n",
    "    params = {'esperar': {'xpath': '//div[@class=\"v-card__text\"][contains(text(), \"Please stand by\")]'}, 'tipo_espera': 'invisibilidad'}\n",
    "    tests.esperar_elemento(params)\n",
    "\n",
    "    # Guardamos el contenido HTML, que nos servirá posteriormente para detectar otros posibles xpaths\n",
    "    # Sustituimos valores de fecha y hora, ya que podría hacer que parezca que dos etiquetas son distintas cuando la diferencia es la hora actual\n",
    "    contenido_html = re.sub(r'\\d+:\\d+|\\d+\\/\\d+\\/\\d+', '', tests.driver.page_source)\n",
    "    contenido_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', contenido_html)\n",
    "\n",
    "    salir = False\n",
    "    while salir is False:\n",
    "        try:\n",
    "            # Preguntamos al usuario por el xpath a buscar\n",
    "            xpath_a_buscar = input('Introduzca el xpath a buscar, o escriba salir para finalizar: ')\n",
    "            \n",
    "            if 'salir' in xpath_a_buscar:\n",
    "                salir = True\n",
    "                break\n",
    "            \n",
    "            # Si el xpath indicado no acaba en //*, se lo añadimos para que busque el contenido dentro del tag\n",
    "            if xpath_a_buscar.endswith('//*') is False:\n",
    "                xpath_a_buscar = xpath_a_buscar + '//*'\n",
    "            \n",
    "            # Buscamos todos los elementos que haya en la página a partir del xpath dado\n",
    "            elementos_encontrados = tests.driver.find_elements(By.XPATH, xpath_a_buscar)\n",
    "\n",
    "            tags_a_eliminar = ['html', 'link', 'title', 'style', 'script', 'head', 'meta']\n",
    "            elementos_encontrados = [elemento for elemento in elementos_encontrados if elemento.tag_name not in tags_a_eliminar]\n",
    "\n",
    "            for elemento in tqdm(elementos_encontrados, unit='xpath', total=len(elementos_encontrados)):\n",
    "                try:\n",
    "                    xpath_a_aniadir = obtener_xpath_de_elemento(elemento)\n",
    "\n",
    "                    if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():\n",
    "                        try:\n",
    "                            contenido_html_sin_separar = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elemento.get_attribute('outerHTML'))\n",
    "                            xpaths_contenido_html = obtener_xpaths_de_html(contenido_html_sin_separar)\n",
    "\n",
    "                            # Obtenemos las palabras clave, que serán de utilidad para poder buscar los xpaths necesarios\n",
    "                            palabras_clave = re.findall(r'\\b\\w+\\b', xpath_a_aniadir)\n",
    "                            palabras_clave.append(re.sub(r'\\d+\\/\\d+\\/\\d+|\\d{2}:\\d{2}', '', elemento.text).replace('\\n\\n', ',').replace('\\n', ','))\n",
    "\n",
    "                            j = 10\n",
    "                            while j < len(palabras_clave):\n",
    "                                palabras_clave.insert(j, '\\n')\n",
    "\n",
    "                                j = j + 10\n",
    "\n",
    "                            # Eliminamos los valores repetidos, ya que en este caso no es necesario tener valores duplicados\n",
    "                            palabras_clave = list(set(palabras_clave))\n",
    "                            palabras_clave = ','.join(palabras_clave)\n",
    "\n",
    "                            # Guardamos la imagen en un fichero, cuyo nombre será el del índice si se hizo captura\n",
    "                            if type(elemento.screenshot_as_png) is bytes:\n",
    "                                nombre_fichero_imagen = os.getcwd() + '\\\\capturas\\\\captura' + str(len(df_xpaths)) + '.png'\n",
    "                                elemento.screenshot(nombre_fichero_imagen)\n",
    "                                df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, nombre_fichero_imagen, xpath_a_buscar, palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                            else:\n",
    "                                df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', xpath_a_buscar,  palabras_clave, contenido_html_sin_separar,  '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                        except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                            df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', xpath_a_buscar,  palabras_clave, contenido_html_sin_separar, '', '']\n",
    "\n",
    "                except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                    pass\n",
    "\n",
    "            # Convertimos contenido_html a lista, pues posteriormente se utilizará para comprobar si acciones como un clic hace que se muestre nuevo contenido\n",
    "            contenido_html = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elementos_encontrados[0].get_attribute('outerHTML'))\n",
    "            contenido_html = contenido_html.split('><')\n",
    "            contenido_html = [('<' + contenido) if contenido.startswith('<') is False else contenido for contenido in contenido_html]\n",
    "            contenido_html = [(contenido + '>') if contenido.endswith('>') is False else contenido for contenido in contenido_html]\n",
    "\n",
    "            # Eliminamos aquellos elementos que puedan considerarse repetidos\n",
    "            j = 0\n",
    "            while j < len(datos_elementos_no_repetibles):\n",
    "                k = 0\n",
    "                while k < len(contenido_html):\n",
    "                    if contenido_html[k] in datos_elementos_no_repetibles[j]:\n",
    "                        datos_elementos_no_repetibles.remove(datos_elementos_no_repetibles[j])\n",
    "\n",
    "                    else:\n",
    "                        k = k + 1\n",
    "\n",
    "                j = j + 1\n",
    "\n",
    "            for contenido in contenido_html:\n",
    "                xpath_a_aniadir = obtener_xpath_de_html(contenido)\n",
    "\n",
    "                if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():\n",
    "                    try:\n",
    "                        # Buscamos todos los elementos que haya en la página\n",
    "                        elementos_encontrados = tests.driver.find_elements(By.XPATH, xpath_a_aniadir + '//*')\n",
    "                        elementos_encontrados = [elemento for elemento in elementos_encontrados if elemento.tag_name not in tags_a_eliminar]\n",
    "\n",
    "                        for elemento in elementos_encontrados:\n",
    "                            try:\n",
    "                                xpath_a_aniadir = obtener_xpath_de_elemento(elemento)\n",
    "\n",
    "                                if xpath_a_aniadir not in df_xpaths['XPATH'].tolist():# and '[' in xpath_a_aniadir:\n",
    "                                    try:\n",
    "                                        contenido_html_sin_separar = re.sub(r'\\sdata[a-zA-Z0-9\\-]{1,}=\"\"|<!---->', '', elemento.get_attribute('outerHTML'))\n",
    "                                        xpaths_contenido_html = obtener_xpaths_de_html(contenido_html_sin_separar)\n",
    "\n",
    "                                        # Obtenemos las palabras clave, que serán de utilidad para poder buscar los xpaths necesarios\n",
    "                                        palabras_clave = re.findall(r'\\b\\w+\\b', xpath_a_aniadir)\n",
    "                                        palabras_clave.append(re.sub(r'\\d+\\/\\d+\\/\\d+|\\d{2}:\\d{2}', '', elemento.text).replace('\\n\\n', ',').replace('\\n', ','))\n",
    "\n",
    "                                        j = 10\n",
    "                                        while j < len(palabras_clave):\n",
    "                                            palabras_clave.insert(j, '\\n')\n",
    "\n",
    "                                            j = j + 10\n",
    "\n",
    "                                        # Eliminamos los valores repetidos, ya que en este caso no es necesario tener valores duplicados\n",
    "                                        palabras_clave = list(set(palabras_clave))\n",
    "                                        palabras_clave = ','.join(palabras_clave)\n",
    "\n",
    "                                        # Guardamos la imagen en un fichero, cuyo nombre será el del índice si se hizo captura\n",
    "                                        if type(elemento.screenshot_as_png) is bytes:\n",
    "                                            nombre_fichero_imagen = os.getcwd() + '\\\\capturas\\\\captura' + str(len(df_xpaths)) + '.png'\n",
    "                                            df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, nombre_fichero_imagen, '', palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                                            # Guardamos la imagen en un fichero, cuyo nombre será el del índice\n",
    "                                            elemento.screenshot(nombre_fichero_imagen)\n",
    "\n",
    "                                    except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException):\n",
    "                                        df_xpaths.loc[len(df_xpaths)] = [xpath_a_aniadir, '', '', palabras_clave, contenido_html_sin_separar, '\\n'.join(xpaths_contenido_html), '']\n",
    "\n",
    "                            except (selenium.common.exceptions.WebDriverException, selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.StaleElementReferenceException, selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                                pass\n",
    "\n",
    "                    except (selenium.common.exceptions.NoSuchElementException, selenium.common.exceptions.InvalidSelectorException, selenium.common.exceptions.ElementClickInterceptedException):\n",
    "                        pass\n",
    "\n",
    "            # Guardamos el fichero Excel\n",
    "            guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # Guardamos fichero pickle para, en caso de volver a ejecutar, poder hacerlo desde donde nos hubiésemos quedado\n",
    "            with open('df_xpaths.pickle', mode='wb') as fichero_pickle:\n",
    "                pickle.dump(df_xpaths, fichero_pickle)\n",
    "\n",
    "            # Acciones como CTRL+C provocan este error. Se captura para poder guardar la información obtenida\n",
    "            print('Se ha detectado la ejecución de CTRL+C. Guardando hasta lo obtenido')\n",
    "            guardar_df_como_excel(df_xpaths, os.getcwd() + '\\\\ficheros_excel\\\\xpaths.xlsx', contiene_imagenes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b78cb-5403-4c2a-8ee9-f9d17c26da03",
   "metadata": {},
   "source": [
    "En la siguiente celda se podrá elegir el método de obtención de xpaths deseados. Una vez elegido, se procederá a la extracción hasta su finalización, ya sea por haber terminado el proceso en el caso del método voraz o por finalización por parte del usuario para cualquiera de los métodos de extracción de información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980299c-05b2-4c41-b087-2f8bff1a8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo_obtencion = input('Indique el tipo de obtención de XPATHS deseado:\\n1) Voraz\\n2) Asistido\\n Opción: ')\n",
    "\n",
    "match tipo_obtencion:\n",
    "    case '1':\n",
    "        obtener_xpaths_voraz()\n",
    "\n",
    "    case '2':\n",
    "        obtener_xpaths_asistido()\n",
    "\n",
    "# Cerramos la sesión\n",
    "tests.cerrar_sesion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db56e38-0e76-4d1c-998d-55485de482c4",
   "metadata": {},
   "source": [
    "A continuación se abrirá el CSV que contiene todos los casos de prueba, sobre el que se añadirán las columnas de posibles índices de datos y sugerencias. Esto busca la facilitación de la redacción de las clases o valores de salida, es decir, la creación del YAML correspondiente a cada caso de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a7ba954-b786-4a2e-8ec6-a6970470cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_posiciones_datos_y_sugerencias():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "    \n",
    "    # Abrimos el fichero Excel que contendrá los xpaths asociados, pues a través de la columna de palabras clave comprobaremos si se puede corresponder a un caso de prueba\n",
    "    df_xpaths = pd.read_excel(io=directorio_ficheros_excel + 'xpaths.xlsx', sheet_name='xpaths', index_col=None)\n",
    "\n",
    "    # Detectamos la codificación del fichero, para así evitar problemas de acentos y otros caracteres especiales\n",
    "    codificacion_detectada = None\n",
    "    with open(directorio_ficheros_excel + 'MODELO.csv', mode='rb') as f:\n",
    "        contenido_fichero = f.read()\n",
    "        codificacion_detectada = chardet.detect(contenido_fichero)['encoding']\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # Una vez detectada, abrimos el CSV que contiene nuestro modelo\n",
    "    df_modelo = pd.read_csv(directorio_ficheros_excel + 'MODELO.csv', sep=';', encoding=codificacion_detectada, index_col=None)\n",
    "    \n",
    "    # Añadimos una columna en el DataFrame del modelo que indicará posibles filas en que se encuentren el/los xpaths que buscamos para el caso de prueba\n",
    "    # Añadiremos además otra con sugerencias\n",
    "    df_modelo['POSIBLES_INDICES_DATOS'] = [''] * len(df_modelo)\n",
    "    df_modelo['SUGERENCIAS'] = [''] * len(df_modelo)\n",
    "\n",
    "    # Creamos una lista que contiene palabras a eliminar del listado de palabras del texto de entrada\n",
    "    palabras_eliminables = ['DATOS', 'Y', 'O', 'EN', 'PARA', 'CALIDAD', 'DE', 'PIE', 'CABECERA', 'MOSTRADOS']\n",
    "\n",
    "    # Creamos a continuación una lista con palabras a partir de las cuales crear las sugerencias\n",
    "    sugerencias = [\n",
    "        'pulsar|Pulsar|clic|seleccionar|Seleccionar',\n",
    "        'tooltip|tool-tip|hover',\n",
    "        'aparece|contiene|Constitución|constitucion',\n",
    "        ' habilitado|deshabilitado'\n",
    "        ]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df_modelo):\n",
    "        # Tanto las sugerencias como la búsqueda de posibles índices que contengan los datos se harán para aquellas filas donde el valor de la columna Tipo es Case\n",
    "        if df_modelo.loc[i]['Tipo'] == 'Case' and df_modelo.loc[i]['YAML'] is not np.NaN:\n",
    "            # Buscamos en primer lugar los posibles índices en que pueda encontrarse lo que nos interesa\n",
    "            palabras = list(set(re.findall(r'\\b[\\w|0-9|\\.|\\\\p{L}\\\\p{M}\\\\p{N}]+\\b|\".+\"', df_modelo.loc[i]['Test'])))\n",
    "            palabras = [re.sub(r'\\(|\\)', '', palabra) for palabra in palabras if palabra.upper() not in palabras_eliminables]\n",
    "\n",
    "            indices_xpaths_encontrados = df_xpaths.loc[df_xpaths['PALABRAS CLAVE'].str.contains('|'.join(palabras))].index.astype('str').tolist()\n",
    "            if len(indices_xpaths_encontrados) > 0:\n",
    "\n",
    "                # En caso de que el número de índices encontrados sea mayor a 30, insertamos retorno de carro cada 30 índices para una mejora de la visualización del fichero final\n",
    "                if len(indices_xpaths_encontrados) > 30:\n",
    "                    indices_xpaths_encontrados = ','.join(indices_xpaths_encontrados).split(',')\n",
    "                    j = 30\n",
    "                    while j < len(indices_xpaths_encontrados):\n",
    "                        indices_xpaths_encontrados.insert(j, '\\r\\n')\n",
    "\n",
    "                        j = j + 30\n",
    "\n",
    "                df_modelo.at[i, 'POSIBLES_INDICES_DATOS'] = ','.join(indices_xpaths_encontrados)\n",
    "\n",
    "            # Buscamos ahora rellenar la columna de sugerencias que puedan facilitar la tarea. Para ello, crearemos una lista con tuplas como contenido\n",
    "            sugerencias_test = []\n",
    "            texto_test = df_modelo.loc[i]['Test']\n",
    "\n",
    "            for sugerencia in sugerencias:\n",
    "                resultado_busqueda = re.search(sugerencia, texto_test)\n",
    "                if resultado_busqueda is not None:\n",
    "                    match sugerencia:\n",
    "                        case 'pulsar|Pulsar|clic|seleccionar|Seleccionar':\n",
    "                            if 'botón' in texto_test or 'icono' in texto_test:\n",
    "                                if 'refresco' in texto_test or 'refrescar' in texto_test or 'actualizar' in texto_test:\n",
    "                                    sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //*[name()=\"path\"][contains(@d, \"M19,8L15\")]')\n",
    "\n",
    "                                elif 'configuración' in texto_test:\n",
    "                                    sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //*[name()=\"path\"][contains(@d, \"M12,15.5A3\")]')\n",
    "\n",
    "                                elif 'añadir' in texto_test:\n",
    "                                    sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //*[name()=\"path\"][contains(@d, \"M19,13H13\")]')\n",
    "\n",
    "                                elif 'papelera' in texto_test or 'borrar' in texto_test:\n",
    "                                    sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //*[name()=\"path\"][contains(@d, \"M9,3V4H4\")]')\n",
    "\n",
    "                                else:\n",
    "                                    resultado_busqueda = re.search(r'botón(\"\\w+\"|\\(.+\\))', texto_test)\n",
    "                                    if resultado_busqueda is not None:\n",
    "                                        resultado_busqueda = resultado_busqueda.group(0)\n",
    "                                        resultado_busqueda = resultado_busqueda.replace('\"', '').replace('()', '').replace(')', '')\n",
    "                                        sugerencias_test.append('  - funcion: hacer_clic\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + resultado_busqueda + '\")]')\n",
    "\n",
    "                        case 'tooltip|tool-tip|hover':\n",
    "                            if '• ' in texto_test:\n",
    "                                texto_aniadible = '  - funcion: comprobar_contenido_hover\\r\\n    params:\\r\\n      - xpath: \\r\\n        esperar: \\r\\n          - xpath: \\r\\n        tipo_comprobacion: y\\r\\n        contenido: '\n",
    "                                if '\\r\\n' in texto_test:\n",
    "                                    lineas_texto = texto_test.split('\\r\\n')\n",
    "\n",
    "                                else:\n",
    "                                    lineas_texto = texto_test.split('\\n')\n",
    "\n",
    "                                j = 0\n",
    "                                while j < len(lineas_texto):\n",
    "                                    if '•' in lineas_texto[j] or '-' in lineas_texto[j]:\n",
    "                                        texto_a_aniadir = lineas_texto[j].replace('•', '').replace('-', '')\n",
    "                                        while texto_a_aniadir.startswith(' '):\n",
    "                                            texto_a_aniadir = texto_a_aniadir[1:]\n",
    "\n",
    "                                        texto_aniadible = texto_aniadible + '\\r\\n          - xpath: //*[contains(text(), \"' + texto_a_aniadir + '\")]'\n",
    "\n",
    "                                        j = j + 1\n",
    "\n",
    "                                    else:\n",
    "                                        lineas_texto.remove(lineas_texto[j])\n",
    "\n",
    "                                sugerencias_test.append(texto_aniadible)\n",
    "\n",
    "                            elif 'mensaje:' in texto_test or 'texto:' in texto_test or 'descripción:' in texto_test:\n",
    "                                resultado_busqueda = re.search(r'(?:mensaje\\:|texto\\:|descripción\\:).+\\.{0,1}$', texto_test)\n",
    "                                if resultado_busqueda is not None:\n",
    "                                    resultado_busqueda = resultado_busqueda.group(0).replace('mensaje: ', '').replace('texto: ', '').replace('descripción: ', '').replace('\"', '').replace('.', '')\n",
    "                                    sugerencias_test.append('  - funcion: comprobar_contenido_hover\\r\\n    params:\\r\\n      - xpath: \\r\\n        esperar: \\r\\n          - xpath: \\r\\n        tipo_comprobacion: y\\r\\n        contenido: \\r\\n          - xpath: //*[contains(text(), \"' + resultado_busqueda + '\")]')\n",
    "\n",
    "                        case 'aparece|contiene|Constitución|constitucion':\n",
    "                            texto_aniadible = '  - funcion: comprobar_contenido_elemento_2\\r\\n    params:\\r\\n      - xpath: \\r\\n        tipo_comprobacion: y\\r\\n        contenido: '\n",
    "                            if 'texto \"' in texto_test:\n",
    "                                textos = re.findall('texto \"\\w+\"', texto_test)\n",
    "                                j = 0\n",
    "                                while j < len(textos):\n",
    "                                    texto_aniadible = texto_aniadible + '\\r\\n          - xpath: //*[contains(text(), \"' + textos[j].replace('\"', '') + '\")]'\n",
    "                                    j = j + 1\n",
    "\n",
    "                                sugerencias_test.append(texto_aniadible)\n",
    "\n",
    "                            if 'el campo' in texto_test or 'los campos' in texto_test:\n",
    "                                textos = re.findall(r'[A-Z]{2,}[A-Z]{2,}|\\\".+\\\"', texto_test)\n",
    "                                j = 0\n",
    "                                while j < len(textos):\n",
    "                                    texto_aniadible = texto_aniadible + '\\r\\n          - xpath: //*[contains(text(), \"' + textos[j].replace('\"', '') + '\")]//following-sibling::input'\n",
    "                                    j = j + 1\n",
    "\n",
    "                                sugerencias_test.append(texto_aniadible)\n",
    "\n",
    "                        case 'habilitado|habilita':\n",
    "                            if 'botón' in texto_test:\n",
    "                                resultado_busqueda = re.search(r'botón\\s[A-Z]+.+(?:deshabilitado|\\shabilitado)', texto_test)\n",
    "                                if resultado_busqueda is not None:\n",
    "                                    resultado_busqueda = resultado_busqueda.group(0)\n",
    "                                    if ' habilitado' in resultado_busqueda:\n",
    "                                        sugerencias_test.append('  - funcion: comprobar_elemento_habilitado\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + resultado_busqueda.split()[1] + '\")]\\r\\n        habilitado: true')\n",
    "\n",
    "                                    else:\n",
    "                                        sugerencias_test.append('  - funcion: comprobar_elemento_habilitado\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + resultado_busqueda.split()[1] + '\")]\\r\\n        habilitado: false')\n",
    "\n",
    "                                else:\n",
    "                                    resultado_busqueda = re.search(r'(?:\"\\w+\"|\\(.+\\)).+(?:habilitado)', texto_test)\n",
    "                                    if resultado_busqueda is not None:\n",
    "                                        resultado_busqueda = resultado_busqueda.group(0)\n",
    "                                        texto_boton = re.search(r'(\"\\w+\"|\\(.+\\))', texto_test)\n",
    "                                        if texto_boton is not None:\n",
    "                                            texto_boton = texto_boton.group(0)\n",
    "                                            texto_boton = texto_boton.replace('\"', '').replace('()','').replace(')', '')\n",
    "\n",
    "                                            if ' habilitado' in resultado_busqueda:\n",
    "                                                sugerencias_test.append('  - funcion: comprobar_elemento_habilitado\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + texto_boton + '\")]\\r\\n        habilitado: true')\n",
    "\n",
    "                                            else:\n",
    "                                                sugerencias_test.append('  - funcion: comprobar_elemento_habilitado\\r\\n    params:\\r\\n      - xpath: //button//*[contains(text(), \"' + texto_boton + '\")]\\r\\n        habilitado: false')\n",
    "\n",
    "            # Se eliminan aquellos elementos de la lista que no tengan contenido\n",
    "            while '' in sugerencias_test:\n",
    "                sugerencias_test.remove('')\n",
    "\n",
    "            if len(sugerencias_test) > 0:\n",
    "                df_modelo.at[i, 'SUGERENCIAS'] = '\\r\\n'.join(list(set(sugerencias_test)))\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Guardamos como fichero XLSX con filtrado y preparado para su edición\n",
    "    guardar_df_como_excel(df_modelo, directorio_ficheros_excel + '\\\\MODELO.xlsx', aplicar_colorizacion=False)\n",
    "\n",
    "obtener_posiciones_datos_y_sugerencias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887fa73-b451-44e7-90ed-7077a297cf4a",
   "metadata": {},
   "source": [
    "Una vez completado el proceso, se deberá completar la columna de YAML, contenedora de las clases de las diferentes instancias. Para ello, el fichero generado a partir de la exploración de la página web y las columnas de sugerencias y posibles índices serán de ayuda en esta parte del proceso.\n",
    "\n",
    "Una vez se complete el proceso, se procederá a la agrupación de casos de prueba en el test correspondiente, de manera que una vez se entrene el modelo sea posible realizar pruebas particulares o pruebas más globales. Si, por ejemplo, se tuviese un test denominado NTP y unos casos de prueba como comprobación de stratum y otro de comprobación de IP, se podría comprobar el stratum, la IP o ambos. A continuación puede verse el código utilizado para dicho propósito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303e435-cae5-4891-b1e5-1ba4dbcdc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupar_casos_en_test():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "    df_modelo = pd.read_excel(io=directorio_ficheros_excel + 'MODELO.xlsx', sheet_name='MODELO', index_col=None)\n",
    "\n",
    "    # Obtenemos los títulos sin valores repetidos\n",
    "    titulos = df_modelo['Titulo'].unique()\n",
    "    for titulo in titulos:\n",
    "        # Obtenemos los YAML de todos los casos para ese test\n",
    "        yamls = df_modelo.loc[(df_modelo['Titulo'].str.contains(titulo)) & (df_modelo['Tipo'].str.contains('Case'))]['YAML']\n",
    "\n",
    "        # Eliminamos aquellos que únicamente tengan el esqueleto\n",
    "        yamls = [yaml for yaml in yamls if 'http' in yaml]\n",
    "\n",
    "        # Obtenemos el índice del Test con dicho título\n",
    "        indice_test = df_modelo.loc[(df_modelo['Titulo'].str.contains(titulo)) & (df_modelo['Tipo'].str.contains('Test'))]\n",
    "        if len(indice_test) > 0:\n",
    "            indice_test = indice_test.index.tolist()[0]\n",
    "\n",
    "            # Insertamos para dicho test\n",
    "            df_modelo.at[indice_test, 'YAML'] = '[\\r\\n' + ',\\r\\n\\r\\n'.join(yamls) + '\\r\\n]'\n",
    "\n",
    "    # Guardamos el modelo con los cambios\n",
    "    guardar_df_como_excel(df_modelo, directorio_ficheros_excel + 'MODELO.xlsx', aplicar_colorizacion=False)\n",
    "\n",
    "agrupar_casos_en_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c9abd-e65e-4d03-b248-07f9b371f22d",
   "metadata": {},
   "source": [
    "En la siguiente celda descargaremos los modelosHelsinki-NLP/opus-mt-es-en y Helsinki-NLP/opus-mt-en-es, que servirán para realizar Data Augmentation mediante la técnica de back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2e241-30c9-45a5-a907-846a8e35d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(repo_id='Helsinki-NLP/opus-mt-es-en', cache_dir='./huggingface_mirror')\n",
    "snapshot_download(repo_id='Helsinki-NLP/opus-mt-en-es', cache_dir='./huggingface_mirror')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b12357-7f70-4895-aef6-a12330c42e64",
   "metadata": {},
   "source": [
    "Una vez se tiene un primer modelo, se han de incrementar el número de atributos para una misma clase o textos para una misma etiqueta, es decir, incrementar el número de textos que producen el mismo YAML de salida, para que así sea más posible que una vez entrenado el modelo se entiendan mejor las acciones que desea realizar el usuario. Para ello, se utilizarán técnicas de Data Augmentation con la librería nlpaug, diseñada para la realización de la tarea de forma sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbec815f-6dfc-4b20-abfa-da635dfbcd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice de inicio: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████▊| 1037/1039 [14:24:26<01:33, 46.52s/texto]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha finalizado la aumentación de datos. Guardando el modelo aumentado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████▊| 1037/1039 [14:24:27<01:40, 50.02s/texto]\n"
     ]
    }
   ],
   "source": [
    "def aumentar_datos_tests_nlpaug():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "    \n",
    "    # Se comprueba en primer lugar si se trata de retomar la tarea o de comenzar desde el principio, en cuyo caso existirá un fichero pickle guardado\n",
    "    try:\n",
    "        with open('df_modelo.pickle', mode='rb') as fichero_pickle:\n",
    "            df_modelo = pickle.load(fichero_pickle)\n",
    "            df_modelo_aumentado = pickle.load(fichero_pickle)\n",
    "            i = pickle.load(fichero_pickle)\n",
    "            \n",
    "            # De alcanzarse este punto sin errores, es que se trata de retomar la tarea. Se inicia el contador\n",
    "            print('Índice de inicio:', i)\n",
    "\n",
    "    # Si se produce un error, se debe a que no existe el fichero. Lo capturamos y no hacemos nada\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        print('No existe el fichero df_modelo.pickle o se ha producido un error. Se comienza desde el inicio.')\n",
    "        \n",
    "        # Se abre el fichero Excel que contiene el modelo base\n",
    "        df_modelo = pd.read_excel(io=directorio_ficheros_excel + 'MODELO.xlsx', sheet_name='MODELO', index_col=None)\n",
    "        i = 0\n",
    "        \n",
    "        # Creamos un dataframe auxiliar, de manera que el test original y aquellos generados con la aumentación queden consecutivos\n",
    "        df_modelo_aumentado = pd.DataFrame(columns=df_modelo.columns)\n",
    "\n",
    "    # Definimos el aumentador de sinónimos\n",
    "    aumentador_sinonimos = aumentador_palabras.SynonymAug(aug_src='wordnet', lang='spa')\n",
    "\n",
    "    # Definimos el aumentador mediante el método de back translation\n",
    "    aumentador_back_translation = aumentador_palabras.back_translation.BackTranslationAug(from_model_name='Helsinki-NLP/opus-mt-es-en', to_model_name='Helsinki-NLP/opus-mt-en-es', name='BackTranslationAug', device='cpu', batch_size=32, max_length=512, force_reload=False, verbose=0)\n",
    "\n",
    "    try:\n",
    "        # Obtenemos en el siguiente bucle todas las filas, e iremos aplicando la técnica fila a fila\n",
    "        barra_de_progreso = tqdm(unit='texto', total=len(df_modelo)-i)\n",
    "        while i < len(df_modelo):\n",
    "            # Guardamos una copia en el archivo df_modelo.pickle para, en caso de fallo, poder retomar a partir de un punto de guardado\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                with open('df_modelo.pickle', mode='wb') as fichero_pickle:\n",
    "                    pickle.dump(df_modelo, fichero_pickle)\n",
    "                    pickle.dump(df_modelo_aumentado, fichero_pickle)\n",
    "                    pickle.dump(i+1, fichero_pickle)\n",
    "            \n",
    "            # En primer lugar añadimos la fila existente al modelo final\n",
    "            df_modelo_aumentado.loc[len(df_modelo_aumentado)] = df_modelo.loc[i]\n",
    "    \n",
    "            # Obtenemos el texto a partir del cual se generarán otros\n",
    "            texto_test = df_modelo.loc[i]['Test']\n",
    "    \n",
    "            # Generamos los nuevos textos, primero mediante aumentación por sinónimos\n",
    "            nuevo_texto = aumentador_sinonimos.augment(texto_test)\n",
    "    \n",
    "            # Añadimos los nuevos textos al modelo final\n",
    "            nueva_fila = df_modelo.loc[i]\n",
    "            nueva_fila['Test'] = nuevo_texto.replace('[', '').replace(']', '')\n",
    "            \n",
    "            df_modelo_aumentado.loc[len(df_modelo_aumentado)] = nueva_fila\n",
    "            \n",
    "            # Generamos nuevos textos mediante la ténica de back translation, generando 4 nuevos textos a partir del original\n",
    "            nuevo_texto = aumentador_back_translation.augment(texto_test, n=4, num_thread=4)\n",
    "    \n",
    "            # Añadimos los nuevos textos al modelo final\n",
    "            nueva_fila = df_modelo.loc[i]\n",
    "            j = 0\n",
    "            while j < len(nuevo_texto):\n",
    "                nueva_fila['Test'] = nuevo_texto[j]\n",
    "                \n",
    "                df_modelo_aumentado.loc[len(df_modelo_aumentado)] = nueva_fila\n",
    "                j = j + 1\n",
    "\n",
    "            barra_de_progreso.update(1)\n",
    "            i = i + 1\n",
    "\n",
    "        print('Se ha finalizado la aumentación de datos. Guardando el modelo aumentado')\n",
    "        \n",
    "        # Guardamos el modelo con los cambios\n",
    "        guardar_df_como_excel(df_modelo_aumentado, directorio_ficheros_excel + 'MODELO_AUMENTADO.xlsx', aplicar_colorizacion=False)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Se ha detectado la ejecución de Ctrl+C. Guardando hasta lo realizado')\n",
    "        with open('df_modelo.pickle', mode='wb') as fichero_pickle:\n",
    "            pickle.dump(df_modelo, fichero_pickle)\n",
    "            pickle.dump(df_modelo_aumentado, fichero_pickle)\n",
    "            pickle.dump(i+1, fichero_pickle)\n",
    "\n",
    "aumentar_datos_tests_nlpaug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d13d55-bec6-42ec-b6d0-6a674cdb7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "df_modelo_aumentado = pd.read_excel(io=directorio_ficheros_excel + 'MODELO_AUMENTADO.xlsx', sheet_name='MODELO_AUMENTADO', index_col=None)\n",
    "df_modelo_aumentado_final = pd.DataFrame(columns=df_modelo_aumentado.columns)\n",
    "\n",
    "tests = df_modelo_aumentado['Test'].tolist()\n",
    "i = 0\n",
    "while i < len(tests):\n",
    "    if tests[i].startswith('['):\n",
    "        lista_tests = tests[i].split(\"', '\")\n",
    "        fila_a_aniadir = df_modelo_aumentado.loc[i]\n",
    "        for elemento in lista_tests:\n",
    "            fila_a_aniadir['Test'] = elemento.replace('[', '').replace(']', '').replace('\\'', '')\n",
    "            df_modelo_aumentado_final.loc[len(df_modelo_aumentado_final)] = fila_a_aniadir\n",
    "\n",
    "        l = 0\n",
    "\n",
    "    else:\n",
    "        df_modelo_aumentado_final.loc[len(df_modelo_aumentado_final)] = df_modelo_aumentado.loc[i]\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "guardar_df_como_excel(df_modelo_aumentado_final, directorio_ficheros_excel + 'MODELO_AUMENTADO.xlsx', aplicar_colorizacion=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78632d3f-3cc1-462d-9528-4ff656854e2f",
   "metadata": {},
   "source": [
    "La siguiente función busca comprobar el resultado de realizar Data Augmentation con ayuda de la librería Transformers, más concretamente con el modelo y tokenizador de MarianMT. Parte del código de la función se encuentra basado en el ejemplo disponible en https://huggingface.co/docs/transformers/model_doc/marian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d11c829-1bfa-4cc0-826a-ddfb0c4b1ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existe el fichero df_modelo_marinamt.pickle o se ha producido un error. Se comienza desde el inicio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████      | 237/257 [22:42<02:19,  6.96s/texto]Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha detectado un texto con más de 512 tokens. Se procede a recortar desde el inicio hasta el máximo permitido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 257/257 [25:41<00:00,  9.29s/texto]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha finalizado la aumentación de datos. Guardando el modelo aumentado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 257/257 [25:42<00:00,  6.00s/texto]\n"
     ]
    }
   ],
   "source": [
    "def aumentar_datos_tests_marianmt():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "\n",
    "    # Se realiza en este punto la importación de modelos y tokenizadores de MarianMT para las traducciones en ambos sentidos\n",
    "    modelo_es_en = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-es-en', max_position_embeddings=512)\n",
    "    modelo_en_es = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-es', max_position_embeddings=512)\n",
    "    tokenizador_marian_es_en = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-es-en', cache_dir='./huggingface_mirror', local_files_only=True, from_pt=True)\n",
    "    tokenizador_marian_en_es = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-es', cache_dir='./huggingface_mirror', local_files_only=True, from_pt=True)\n",
    "    \n",
    "    # Se comprueba en primer lugar si se trata de retomar la tarea o de comenzar desde el principio, en cuyo caso existirá un fichero pickle guardado\n",
    "    try:\n",
    "        with open('df_modelo_marianmt.pickle', mode='rb') as fichero_pickle:\n",
    "            df_modelo = pickle.load(fichero_pickle)\n",
    "            df_modelo_aumentado = pickle.load(fichero_pickle)\n",
    "            i = pickle.load(fichero_pickle)\n",
    "            \n",
    "            # De alcanzarse este punto sin errores, es que se trata de retomar la tarea. Se inicia el contador\n",
    "            print('Índice de inicio:', i)\n",
    "\n",
    "    # Si se produce un error, se debe a que no existe el fichero. Lo capturamos y no hacemos nada\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        print('No existe el fichero df_modelo_marinamt.pickle o se ha producido un error. Se comienza desde el inicio.')\n",
    "        \n",
    "        # Se abre el fichero Excel que contiene el modelo base\n",
    "        df_modelo = pd.read_excel(io=directorio_ficheros_excel + 'MODELO.xlsx', sheet_name='MODELO', index_col=None)\n",
    "        i = 0\n",
    "        \n",
    "        # Creamos un dataframe auxiliar, de manera que el test original y aquellos generados con la aumentación queden consecutivos\n",
    "        df_modelo_aumentado = pd.DataFrame(columns=df_modelo.columns)\n",
    "\n",
    "    # Se procede al borrado de aquellas filas que contienen únicamente el esqueleto, si las hubiese, dado que dichos textos no formarán parte del modelo final\n",
    "    df_modelo = df_modelo.loc[(df_modelo['YAML'] != np.NaN) &(df_modelo['YAML'].str.contains('http'))]\n",
    "\n",
    "    # Se realiza un reseteo de índices para que no se produzcan errores en la obtención de filas\n",
    "    df_modelo.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Se define a continuación un diccionario de sinónimos y qeuivalencias, ya sea una o varias palabras equivalentes entre sí para la aumentación de datos\n",
    "    # El motivo de la generación de este diccionario se basa en la falta de contexto en el intento de aumentación por sinónimos de manera automática\n",
    "    # El análisis de datos que contiene la cantidad de ocurrencias y la revisión manual del modelo inicial fueron de utilidad para la observación de las palabras y expresionesm más utilizadas\n",
    "    diccionario_sinonimos = {\n",
    "        'Comprobar': 'Verificar', 'comprobar': 'verificar',\n",
    "        'Verificar': 'Comprobar', 'verificar': 'comprobar',\n",
    "        'Configuración': 'Conformación', 'configuración': 'conformación',\n",
    "        'área': 'sección', 'Área': 'Sección',\n",
    "        'al pulsar': 'al hacer clic', 'pulsando': 'haciendo clic',\n",
    "        'hacer hover': 'poner el ratón', 'haciendo hover': 'poniendo el ratón',\n",
    "        'Constitución': 'Contenido', 'constitución': 'contenido',\n",
    "        'Contenido': 'Constitución', 'contenido': 'constitución',\n",
    "        'Campo': 'Columna/Elemento', 'campo': 'columna/elemento',\n",
    "        'Columna': 'Campo', 'columna': 'campo',\n",
    "        'Backup': 'Copia de seguridad', 'backup': 'copia de seguridad',\n",
    "        'Máximo': 'Total', 'máximo': 'total',\n",
    "        'Total': 'Máximo', 'total': 'máximo',\n",
    "        'la leyenda mostrada': 'el texto mostrado', 'la calidad': 'la veracidad',\n",
    "        'la veracidad': 'la calidad', 'un tooltip': 'una ventana emergente/un mensaje emergente',\n",
    "        'un tool-tip': 'una ventana emergente/un mensaje emergente'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Obtenemos la cantidad total de instancias o filas del modelo\n",
    "        filas_modelo = len(df_modelo)\n",
    "\n",
    "        # En esta sección se ejecutará la técnica de aumentación de sustitución de caracteres, que la librería easy-nlp-augmentation realiza directamente sobre el dataframe\n",
    "        #clases_a_aumentar = df_modelo['YAML'].unique().tolist()\n",
    "        #df_aumentado = augment_random_character(df_modelo, clases_a_aumentar, augmentation_percentage=1, text_column='Test')\n",
    "        #print(df_modelo)\n",
    "        #print(df_modelo_aumentado)\n",
    "        #print(len(df_modelo))\n",
    "        #print(len(df_aumentado))\n",
    "        #input('parar')\n",
    "        # Obtenemos en el siguiente bucle todas las filas, e iremos aplicando la técnica fila a fila\n",
    "        barra_de_progreso = tqdm(unit='texto', total=filas_modelo-i, position=0)\n",
    "        while i < filas_modelo:\n",
    "            # Guardamos una copia en el archivo df_modelo_marianmt.pickle para, en caso de fallo, poder retomar a partir de un punto de guardado\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                with open('df_modelo_marianmt.pickle', mode='wb') as fichero_pickle:\n",
    "                    pickle.dump(df_modelo, fichero_pickle)\n",
    "                    pickle.dump(df_modelo_aumentado, fichero_pickle)\n",
    "                    pickle.dump(i+1, fichero_pickle)\n",
    "            \n",
    "            # En primer lugar añadimos la fila existente al modelo final\n",
    "            df_modelo_aumentado.loc[len(df_modelo_aumentado)] = df_modelo.loc[i]\n",
    "    \n",
    "            # Obtenemos el texto a partir del cual se generarán otros\n",
    "            texto_test = df_modelo.loc[i]['Test']\n",
    "\n",
    "            # Se procede en este punto a la traducción, en primer lugar de español a inglés y posteriormente de ingés a español\n",
    "            # De español castellano a inglés\n",
    "            try:\n",
    "                traduccion_es_en = modelo_es_en.generate(**tokenizador_marian_es_en(texto_test, return_tensors=\"pt\", padding=True))\n",
    "                texto_traduccion = [tokenizador_marian_es_en.decode(traduccion, skip_special_tokens=True) for traduccion in traduccion_es_en]\n",
    "\n",
    "            except IndexError:\n",
    "                print('Se ha detectado un texto con más de 512 tokens. Se procede a recortar desde el inicio hasta el máximo permitido.')\n",
    "                # El error es debido a que supera el numero máximo de tokens (512). De ser así, se obtiene hasta el máximo, eliminando el resto\n",
    "                texto_tokenizado = tokenizador_marian_es_en.tokenize(texto_test)\n",
    "        \n",
    "                texto_tokenizado = texto_tokenizado[:511]\n",
    "                texto_test = ''.join(texto_tokenizado).replace('▁', ' ')\n",
    "                traduccion_es_en = modelo_es_en.generate(**tokenizador_marian_es_en(texto_test, return_tensors=\"pt\", padding=True))\n",
    "                texto_traduccion = [tokenizador_marian_es_en.decode(traduccion, skip_special_tokens=True) for traduccion in traduccion_es_en]\n",
    "                \n",
    "            # De inglés a español castellano\n",
    "            traduccion_en_es = modelo_en_es.generate(**tokenizador_marian_en_es(texto_traduccion, return_tensors=\"pt\", padding=True))\n",
    "            nuevo_texto_back_translation = [tokenizador_marian_en_es.decode(traduccion, skip_special_tokens=True) for traduccion in traduccion_en_es]\n",
    "\n",
    "            # Se modifica el valor de la columna Test para el caso de prueba, introduciéndose el texto aumentado\n",
    "            nueva_fila = df_modelo.loc[i]\n",
    "            nueva_fila['Test'] = nuevo_texto_back_translation[0]\n",
    "            df_modelo_aumentado.loc[len(df_modelo_aumentado)] = nueva_fila\n",
    "\n",
    "            #palabras_expresiones_encontradas = [palabra_expresion for palabra_expresion in palabras_expresiones_texto if palabra_expresion in diccionario_sinonimos.keys()]\n",
    "            claves_diccionario_sinonimos = list(diccionario_sinonimos.keys())\n",
    "\n",
    "            # Se elimina toda palabra o expresión que no se encuentre en la lista de claves\n",
    "            palabras_expresiones_encontradas = [clave for clave in claves_diccionario_sinonimos if clave in texto_test]\n",
    "            \n",
    "            # De haber más de 3 palabras o expresiones, se hace una selección aleatoria de entre 3 y la cantidad de palabras o expresiones encontradas\n",
    "            if palabras_expresiones_encontradas is not None and len(palabras_expresiones_encontradas) > 3:\n",
    "                total_lista = random.randint(3, len(palabras_expresiones_encontradas))\n",
    "                random.shuffle(palabras_expresiones_encontradas)\n",
    "                palabras_expresiones_encontradas = palabras_expresiones_encontradas[:total_lista]\n",
    "                \n",
    "            # Se sustitutyen dichas palabras y expresiones sobre el texto original\n",
    "            nuevo_texto_sinonimos = texto_test\n",
    "            for palabra_expresion in palabras_expresiones_encontradas:\n",
    "                nuevo_texto_sinonimos = nuevo_texto_sinonimos.replace(palabra_expresion, diccionario_sinonimos[palabra_expresion])\n",
    "\n",
    "            if nuevo_texto_sinonimos != texto_test:\n",
    "                nueva_fila['Test'] = nuevo_texto_sinonimos\n",
    "                df_modelo_aumentado.loc[len(df_modelo_aumentado)] = nueva_fila\n",
    "\n",
    "            # Se procede a continuación a realizar data augmentation mediante reemplazo de caracteres. La cantidad de caracteres a reemplazar se determinará de forma aleatoria\n",
    "            texto_test = df_modelo.loc[i]['Test']\n",
    "\n",
    "            # Obtenemos un valor entero aleatorio para determinar el número de caracteres a modificar\n",
    "            caracteres_a_modificar = random.randint(1,3)\n",
    "            while caracteres_a_modificar > 0:\n",
    "                # Se elige de forma aleatoria la posición a modificar\n",
    "                posicion = random.randint(0,len(texto_test)-1)\n",
    "                if texto_test[posicion] >= 'a' and texto_test[posicion] <= 'z':\n",
    "                    minusculas = string.ascii_lowercase\n",
    "                    minusculas = minusculas.replace(texto_test[posicion], '')\n",
    "                    texto_test = texto_test[:posicion] + random.choice(minusculas) + texto_test[posicion+1:]\n",
    "                    caracteres_a_modificar = caracteres_a_modificar - 1\n",
    "                    \n",
    "                elif texto_test[posicion] >= 'A' and texto_test[posicion] <= 'Z':\n",
    "                    mayusculas = string.ascii_uppercase\n",
    "                    mayusculas = mayusculas.replace(texto_test[posicion], '')\n",
    "                    texto_test = texto_test[:posicion] + random.choice(mayusculas) + texto_test[posicion+1:]\n",
    "                    caracteres_a_modificar = caracteres_a_modificar - 1\n",
    "                    \n",
    "            nueva_fila['Test'] = texto_test\n",
    "            df_modelo_aumentado.loc[len(df_modelo_aumentado)] = nueva_fila\n",
    "            \n",
    "            # En este punto se generará un nuevo atributo a partir del existente, a partir del título del caso de prueba y del test\n",
    "            # Se completará y corregirá según corresponda en la revisión manual\n",
    "            texto_test = df_modelo.loc[i]['Test']\n",
    "            tipo = df_modelo.loc[i]['Tipo']\n",
    "            titulo = df_modelo.loc[i]['Titulo']\n",
    "\n",
    "            # Se eliminan espacios que puedan existir al inicio o final\n",
    "            while titulo.startswith(' '):\n",
    "                titulo = titulo[1:]\n",
    "\n",
    "            while titulo.endswith(' '):\n",
    "                titulo = titulo[:-1]\n",
    "            \n",
    "            if tipo == 'Case':\n",
    "                if re.fullmatch(r'^([A-Z]+|\\s{0,1}|Á|É|Í|Ó|Ú)+$', texto_test) is not None:\n",
    "                    nuevo_texto_test = 'Validar ' + texto_test.split('\\n')[0].replace('\\r', '') + ' en ' + titulo.lower()\n",
    "\n",
    "                else:\n",
    "                    nuevo_texto_test = 'Validar ' + texto_test.split('\\n')[0].replace('\\r', '') + ' en ' + titulo.replace(titulo[0], titulo[0].lower())\n",
    "\n",
    "            elif tipo == 'Test':\n",
    "                if re.fullmatch(r'^([A-Z]+|\\s{0,1}|Á|É|Í|Ó|Ú)+$', texto_test) is not None:\n",
    "                    nuevo_texto_test = 'Validar ' + titulo.lower()\n",
    "\n",
    "                else:\n",
    "                    nuevo_texto_test = 'Validar ' + titulo.replace(titulo[0], titulo[0].lower())\n",
    "\n",
    "            # Se añade la nueva fila al modelo final\n",
    "            nueva_fila['Test'] = nuevo_texto_test\n",
    "            df_modelo_aumentado.loc[len(df_modelo_aumentado)] = nueva_fila\n",
    "            \n",
    "            barra_de_progreso.update(1)\n",
    "            i = i + 1\n",
    "\n",
    "        print('Se ha finalizado la aumentación de datos. Guardando el modelo aumentado')\n",
    "        \n",
    "        # Guardamos el modelo con los cambios\n",
    "        guardar_df_como_excel(df_modelo_aumentado, directorio_ficheros_excel + 'MODELO_AUMENTADO_MARIANMT.xlsx', aplicar_colorizacion=False)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Se ha detectado la ejecución de Ctrl+C. Guardando hasta lo realizado')\n",
    "        with open('df_modelo_marianmt.pickle', mode='wb') as fichero_pickle:\n",
    "            pickle.dump(df_modelo, fichero_pickle)\n",
    "            pickle.dump(df_modelo_aumentado, fichero_pickle)\n",
    "            pickle.dump(i+1, fichero_pickle)\n",
    "\n",
    "aumentar_datos_tests_marianmt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3061bd31-8391-4cf6-ac5f-ef5439465f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "df_modelo_aumentado = pd.read_excel(io=directorio_ficheros_excel + 'MODELO_AUMENTADO_MARIANMT.xlsx', sheet_name='MODELO_AUMENTADO_MARIANMT', index_col=None)\n",
    "df_modelo_aumentado_final = pd.DataFrame(columns=df_modelo_aumentado.columns)\n",
    "\n",
    "tests = df_modelo_aumentado['Test'].tolist()\n",
    "i = 0\n",
    "while i < len(tests):\n",
    "    if tests[i].startswith('['):\n",
    "        lista_tests = tests[i].split(\"', '\")\n",
    "        fila_a_aniadir = df_modelo_aumentado.loc[i]\n",
    "        for elemento in lista_tests:\n",
    "            fila_a_aniadir['Test'] = elemento.replace('[', '').replace(']', '').replace('\\'', '')\n",
    "            df_modelo_aumentado_final.loc[len(df_modelo_aumentado_final)] = fila_a_aniadir\n",
    "\n",
    "        l = 0\n",
    "\n",
    "    else:\n",
    "        df_modelo_aumentado_final.loc[len(df_modelo_aumentado_final)] = df_modelo_aumentado.loc[i]\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "    # Eliminamos valores duplicados\n",
    "    df_modelo_aumentado_final.drop_duplicates(inplace=True,ignore_index=True)\n",
    "\n",
    "guardar_df_como_excel(df_modelo_aumentado_final, directorio_ficheros_excel + 'MODELO_AUMENTADO_MARIANMT.xlsx', aplicar_colorizacion=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c43a2-ba3a-4048-bd78-b3f1a27e1ca4",
   "metadata": {},
   "source": [
    "Una vez completado el proceso, se procederá a una revisión del modelo resultante. En primer lugar se comprobará que para clase existen al menos 5 instancias, mostrándose un mensaje de no ser así. De tener todas las clases al menos 5 instancias, se procederá a guardar el modelo que se entrenará, eliminando las columnas que ya no son necesarias. Antes del guardado, se creará una nueva columna de etiquetas referente al tipo o tipo de acciones a realizar (hover o comprobar elementos de una ventana por ejemplo), que se obtendrá a partir del valor de funcion de la columna YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2667a998-9597-41cc-9c1c-a584080e37ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha guardado el modelo de entrenamiento en C:\\Users\\jmarrieta\\PycharmProjects\\TFM_NLP\\ficheros_excel\\MODELO_ENTRENAMIENTO.xlsx\n"
     ]
    }
   ],
   "source": [
    "def crear_etiquetas(yaml):\n",
    "    # Se definen las acciones que no son relevantes\n",
    "    acciones_descartables = ['hacer_clic', 'hacer_si', 'hacer_clic_derecho', 'ir_a_url', 'pulsar_tecla', 'buscar_elemento', 'buscar_elementos', \n",
    "                             'buscar_elementos_hijos', 'buscar_elemento_hijo', 'seleccionar_elemento', 'esperar_elemento', 'ir_a_centro',\n",
    "                             'poner_quitar_pantalla_completa', 'obtener_fecha_y_hora', 'rellenar_campo', 'comprobar_url_actual', 'mover_elemento',\n",
    "                             'iniciar_sesion', 'cerrar_sesion', 'hacer_hover', 'hacer_si_no_existe', 'guardar_valor', 'obtener_valor']\n",
    "\n",
    "    # Se hace una primera separación por líneas\n",
    "    acciones = yaml.split('\\n') \n",
    "    acciones = [accion.replace('\\r', '') for accion in acciones]\n",
    "    \n",
    "    # Se obtienen las acciones\n",
    "    acciones = [accion.split('funcion:')[1] for accion in acciones if 'funcion' in accion]\n",
    "\n",
    "    # Se eliminan los posibles espaciones que pueda contener al inicio del elemento\n",
    "    i = 0\n",
    "    while i < len(acciones):\n",
    "        while acciones[i].startswith(' '):\n",
    "            acciones[i] = acciones[i][1:]\n",
    "            \n",
    "        i = i + 1\n",
    "\n",
    "    # Se eliminan aquellas que se han considerado descartables\n",
    "    acciones = [accion for accion in acciones if accion not in acciones_descartables]\n",
    "\n",
    "    # Se eliminan valores duplicados\n",
    "    acciones = list(set(acciones))\n",
    "    \n",
    "    # Se devuelven las acciones. En caso de que la lista contenga más de un elemento, en primer lugar se ordenan sus elementos\n",
    "    if len(acciones) == 1:\n",
    "        return acciones[0]\n",
    "\n",
    "    elif len(acciones) > 1:\n",
    "        acciones.sort()\n",
    "        return ','.join(acciones)\n",
    "\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def guardar_modelo_entrenamiento():\n",
    "    directorio_ficheros_excel = os.getcwd() + '\\\\ficheros_excel\\\\'\n",
    "    df_modelo = pd.read_excel(io=directorio_ficheros_excel + 'MODELO_AUMENTADO_MARIANMT.xlsx', sheet_name='MODELO_AUMENTADO_MARIANMT', index_col=None)\n",
    "\n",
    "    # Se comprueba si todas las clases contienen 5 instancias\n",
    "    clases = df_modelo['YAML'].unique()\n",
    "\n",
    "    indices = []\n",
    "    for clase in clases:\n",
    "        filas = df_modelo.loc[df_modelo['YAML'] == clase]\n",
    "        if len(filas) < 5:\n",
    "            indices.append(filas.index[0] + 2)\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        print('Se han de revisar las clases de las siguientes filas:\\n', indices)\n",
    "    \n",
    "    else:\n",
    "        # Se procede a eliminar las columnas que originalmente fueron usadas para la edición y generación del modelo y ya no resultan necesarias\n",
    "        df_modelo.drop(columns=['Tipo', 'Titulo', 'SUGERENCIAS', 'POSIBLES_INDICES_DATOS'], inplace=True)\n",
    "\n",
    "        df_modelo['acciones'] = df_modelo['YAML'].apply(crear_etiquetas)\n",
    "        guardar_df_como_excel(df_modelo, directorio_ficheros_excel + 'MODELO_ENTRENAMIENTO.xlsx', aplicar_colorizacion=False)\n",
    "        print('Se ha guardado el modelo de entrenamiento en ' + directorio_ficheros_excel + 'MODELO_ENTRENAMIENTO.xlsx')\n",
    "\n",
    "guardar_modelo_entrenamiento()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72c5f3-78cb-4a64-bb38-7da1fb80c478",
   "metadata": {},
   "source": [
    "En este punto el dataset se encuentra listo para el entrenamiento del modelo. Puede visuarlizarse en **entrenamiento_y_uso_modelo.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
